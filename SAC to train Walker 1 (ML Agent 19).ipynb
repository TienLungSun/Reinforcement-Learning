{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XM51U7nXPhY_"
   },
   "source": [
    "Code Reference: https://github.com/zhihanyang2022/pytorch-sac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "z29hg89Qmg-S"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal, Independent\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from mlagents_envs.environment import UnityEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "U5TAt7Mcmg-W",
    "outputId": "6ef7273c-d02b-45b2-9512-a2e57da25817"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "if(torch.cuda.is_available()):\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(device, torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device= torch.device(\"cpu\")\n",
    "    print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jylhZLcfPhZE"
   },
   "source": [
    "### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3adEaKfdPhZE"
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "\n",
    "Transition = namedtuple('Transition', 's a r ns d')\n",
    "Batch = namedtuple('Batch', 's a r ns d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "nPe_0AuKPhZE",
    "outputId": "a1d2f089-5d1c-4af9-8ac1-de4abbfb2983"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User(name='tester', age='22', id='464643123')\n"
     ]
    }
   ],
   "source": [
    "# example of namedtuple\n",
    "User = namedtuple('User', 'name age id')\n",
    "user = User('tester', '22', '464643123')\n",
    "print(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "U2yli24LPhZF"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, capacity: int):\n",
    "        self.capacity = capacity\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, transition: Transition) -> None:\n",
    "        self.memory.appendleft(transition)\n",
    "\n",
    "    def ready_for(self, batch_size: int) -> bool:\n",
    "        if len(self.memory) >= batch_size:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def sample(self, batch_size: int) -> Batch:\n",
    "        experiences = random.sample(self.memory, batch_size)\n",
    "        s  = torch.from_numpy(np.vstack([e.s for e in experiences if e is not None])).float().to(device)\n",
    "        a  = torch.from_numpy(np.vstack([e.a for e in experiences if e is not None])).float().to(device)\n",
    "        r  = torch.from_numpy(np.vstack([e.r for e in experiences if e is not None])).float().to(device)\n",
    "        ns  = torch.from_numpy(np.vstack([e.ns for e in experiences if e is not None])).float().to(device)\n",
    "        d  = torch.from_numpy(np.vstack([e.d for e in experiences if e is not None])).float().to(device)\n",
    "        return Batch(s, a, r, ns, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "WuR6Cv5TPhZG",
    "outputId": "a6dc64cd-62ba-4a47-f39a-b853dee2b8cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3\n"
     ]
    }
   ],
   "source": [
    "# test buffer\n",
    "b = ReplayBuffer(capacity=5)  \n",
    "b.push(Transition(1,2, 3, 4, 5))\n",
    "print(b.memory[0].a, b.memory[0].r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTfih7jRPhZG"
   },
   "source": [
    "### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-1bX86TnPhZH"
   },
   "outputs": [],
   "source": [
    "def get_net(\n",
    "        num_in:int,\n",
    "        num_out:int,\n",
    "        final_activation,  # e.g. nn.Tanh\n",
    "        num_hidden_layers:int=5,\n",
    "        num_neurons_per_hidden_layer:int=64\n",
    "    ) -> nn.Sequential:\n",
    "\n",
    "    layers = []\n",
    "\n",
    "    layers.extend([\n",
    "        nn.Linear(num_in, num_neurons_per_hidden_layer),\n",
    "        nn.ReLU(),\n",
    "    ])\n",
    "\n",
    "    for _ in range(num_hidden_layers):\n",
    "        layers.extend([\n",
    "            nn.Linear(num_neurons_per_hidden_layer, num_neurons_per_hidden_layer),\n",
    "            nn.ReLU(),\n",
    "        ])\n",
    "\n",
    "    layers.append(nn.Linear(num_neurons_per_hidden_layer, num_out))\n",
    "\n",
    "    if final_activation is not None:\n",
    "        layers.append(final_activation)\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GxCjkEDTPhZH"
   },
   "outputs": [],
   "source": [
    "class NormalPolicyNet(nn.Module):\n",
    "\n",
    "    \"\"\"Outputs a distribution with parameters learnable by gradient descent.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        super(NormalPolicyNet, self).__init__()\n",
    "        self.shared_net   = get_net(num_in=input_dim, num_out=64, final_activation=nn.ReLU())\n",
    "        self.means_net    = nn.Linear(64, action_dim)\n",
    "        self.log_stds_net = nn.Linear(64, action_dim)\n",
    "\n",
    "    def forward(self, states: torch.tensor):\n",
    "\n",
    "        out = self.shared_net(states)\n",
    "        means, log_stds = self.means_net(out), self.log_stds_net(out)\n",
    "\n",
    "        # the gradient of computing log_stds first and then using torch.exp\n",
    "        # is much more well-behaved then computing stds directly using nn.Softplus()\n",
    "        # ref: https://github.com/openai/spinningup/blob/master/spinup/algos/pytorch/sac/core.py#L26\n",
    "\n",
    "        LOG_STD_MAX = 2\n",
    "        LOG_STD_MIN = -20\n",
    "\n",
    "        stds = torch.exp(torch.clamp(log_stds, LOG_STD_MIN, LOG_STD_MAX))\n",
    "\n",
    "        return Independent(Normal(loc=means, scale=stds), reinterpreted_batch_ndims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "d_w1UU3kPhZI"
   },
   "outputs": [],
   "source": [
    "class QNet(nn.Module):\n",
    "\n",
    "    \"\"\"Has little quirks; just a wrapper so that I don't need to call concat many times\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        super(QNet, self).__init__()\n",
    "        self.net = get_net(num_in=input_dim+action_dim, num_out=1, final_activation=None)\n",
    "\n",
    "    def forward(self, states: torch.tensor, actions: torch.tensor):\n",
    "        return self.net(torch.cat([states, actions], dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1-yHHtyPhZI"
   },
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "77zeoV-0PhZI"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "\n",
    "        self.Normal   = NormalPolicyNet(input_dim=input_dim, action_dim=action_dim).to(device)\n",
    "        self.Normal_optimizer = optim.Adam(self.Normal.parameters(), lr=1e-3)\n",
    "\n",
    "        self.Q1       = QNet(input_dim=input_dim, action_dim=action_dim).to(device)\n",
    "        self.Q1_targ  = QNet(input_dim=input_dim, action_dim=action_dim).to(device)\n",
    "        self.Q1_targ.load_state_dict(self.Q1.state_dict())\n",
    "        self.Q1_optimizer = optim.Adam(self.Q1.parameters(), lr=1e-3)\n",
    "\n",
    "        self.Q2       = QNet(input_dim=input_dim, action_dim=action_dim).to(device)\n",
    "        self.Q2_targ  = QNet(input_dim=input_dim, action_dim=action_dim).to(device)\n",
    "        self.Q2_targ.load_state_dict(self.Q2.state_dict())\n",
    "        self.Q2_optimizer = optim.Adam(self.Q2.parameters(), lr=1e-3)\n",
    "\n",
    "        self.gamma = 0.99\n",
    "        self.alpha = 0.1\n",
    "        self.polyak = 0.995\n",
    "\n",
    "    # ==================================================================================================================\n",
    "    # Helper methods (it is generally not my style of using helper methods but here they improve readability)\n",
    "    # ==================================================================================================================\n",
    "\n",
    "    def min_i_12(self, a: torch.tensor, b: torch.tensor) -> torch.tensor:\n",
    "        return torch.min(a, b)\n",
    "\n",
    "    def sample_action_and_compute_log_pi(self, state, use_reparametrization_trick):\n",
    "        mu_given_s = self.Normal(state)  # in paper, mu represents the normal distribution\n",
    "        # in paper, u represents the un-squashed action; nu stands for next u's\n",
    "        # actually, we can just use reparametrization trick in both Step 12 and 14, but it might be good to separate\n",
    "        # the two cases for pedagogical purposes, i.e., using reparametrization trick is a must in Step 14\n",
    "        u = mu_given_s.rsample() if use_reparametrization_trick else mu_given_s.sample()\n",
    "        a = torch.tanh(u)\n",
    "        # the following line of code is not numerically stable:\n",
    "        # log_pi_a_given_s = mu_given_s.log_prob(u) - torch.sum(torch.log(1 - torch.tanh(u) ** 2), dim=1)\n",
    "        # ref: https://github.com/vitchyr/rlkit/blob/0073d73235d7b4265cd9abe1683b30786d863ffe/rlkit/torch/distributions.py#L358\n",
    "        # ref: https://github.com/tensorflow/probability/blob/master/tensorflow_probability/python/bijectors/tanh.py#L73\n",
    "        log_pi_a_given_s = mu_given_s.log_prob(u) - (2 * (np.log(2) - u - F.softplus(-2 * u))).sum(dim=1)\n",
    "        return a, log_pi_a_given_s\n",
    "\n",
    "    def clip_gradient(self, net: nn.Module) -> None:\n",
    "        for param in net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "\n",
    "    def polyak_update(self, old_net: nn.Module, new_net: nn.Module) -> None:\n",
    "        for old_param, new_param in zip(old_net.parameters(), new_net.parameters()):\n",
    "            old_param.data.copy_(old_param.data * self.polyak + new_param.data * (1 - self.polyak))\n",
    "\n",
    "    # ==================================================================================================================\n",
    "    # Methods for learning\n",
    "    # ==================================================================================================================\n",
    "\n",
    "    def update_networks(self, b: Batch) -> None:\n",
    "\n",
    "        # ========================================\n",
    "        # Step 12: calculating targets\n",
    "        # ========================================\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            na, log_pi_na_given_ns = self.sample_action_and_compute_log_pi(b.ns, use_reparametrization_trick=False)\n",
    "            targets = b.r + self.gamma * (1 - b.d) * \\\n",
    "                      (self.min_i_12(self.Q1_targ(b.ns, na), self.Q2_targ(b.ns, na)) - self.alpha * log_pi_na_given_ns)\n",
    "\n",
    "        # ========================================\n",
    "        # Step 13: learning the Q functions\n",
    "        # ========================================\n",
    "\n",
    "        Q1_predictions = self.Q1(b.s, b.a)\n",
    "        Q1_loss = torch.mean((Q1_predictions - targets) ** 2)\n",
    "\n",
    "        self.Q1_optimizer.zero_grad()\n",
    "        Q1_loss.backward()\n",
    "        self.clip_gradient(net=self.Q1)\n",
    "        self.Q1_optimizer.step()\n",
    "\n",
    "        Q2_predictions = self.Q2(b.s, b.a)\n",
    "        Q2_loss = torch.mean((Q2_predictions - targets) ** 2)\n",
    "\n",
    "        self.Q2_optimizer.zero_grad()\n",
    "        Q2_loss.backward()\n",
    "        self.clip_gradient(net=self.Q2)\n",
    "        self.Q2_optimizer.step()\n",
    "\n",
    "        # ========================================\n",
    "        # Step 14: learning the policy\n",
    "        # ========================================\n",
    "\n",
    "        for param in self.Q1.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.Q2.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        a, log_pi_a_given_s = self.sample_action_and_compute_log_pi(b.s, use_reparametrization_trick=True)\n",
    "        policy_loss = - torch.mean(self.min_i_12(self.Q1(b.s, a), self.Q2(b.s, a)) - self.alpha * log_pi_a_given_s)\n",
    "\n",
    "        self.Normal_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.clip_gradient(net=self.Normal)\n",
    "        self.Normal_optimizer.step()\n",
    "\n",
    "        for param in self.Q1.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.Q2.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        # ========================================\n",
    "        # Step 15: update target networks\n",
    "        # ========================================\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.polyak_update(old_net=self.Q1_targ, new_net=self.Q1)\n",
    "            self.polyak_update(old_net=self.Q2_targ, new_net=self.Q2)\n",
    "\n",
    "    def act(self, state)-> np.array:\n",
    "        # state: torch.FloatTensor(s).to(device)\n",
    "        action, _ = self.sample_action_and_compute_log_pi(state, use_reparametrization_trick=False)\n",
    "        return action.detach().cpu().numpy()  # no need to detach first because we are not using the reparametrization trick\n",
    "\n",
    "    def save_actor(self, save_dir: str, filename: str) -> None:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        torch.save(self.Normal.state_dict(), os.path.join(save_dir, filename))\n",
    "\n",
    "    def load_actor(self, save_dir: str, filename: str) -> None:\n",
    "        self.Normal.load_state_dict(torch.load(os.path.join(save_dir, filename)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0qq4KlvPhZL"
   },
   "source": [
    "### Instinate NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "nwL2ePKKPhZL"
   },
   "outputs": [],
   "source": [
    "N_STATES  = 243\n",
    "N_ACTIONS =39\n",
    "\n",
    "N_AGENTS = 3  \n",
    "hidden_units = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "9A-j-EkEPhZL"
   },
   "outputs": [],
   "source": [
    "buf = ReplayBuffer(capacity=int(1e3))   #2M in Walker.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "LypXWR6IPhZL"
   },
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    input_dim=N_STATES,\n",
    "    action_dim=N_ACTIONS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "a7uCTtJMPhZM"
   },
   "outputs": [],
   "source": [
    "batch_size = 64 #1024 in Walker.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_igamj8TPhZM"
   },
   "source": [
    "# Step-by-step test of the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "5zoefMAmPhZM"
   },
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name= None, base_port=5004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "0hmKGllbPhZM",
    "outputId": "6e6dc36c-91bc-4da7-f97f-02dea3b27684"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walker?team=0\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "behaviorNames = list(env.behavior_specs.keys())\n",
    "behaviorName = behaviorNames[0]\n",
    "print(behaviorName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "OxOCjqO2PhZM"
   },
   "outputs": [],
   "source": [
    "DecisionSteps, TerminalSteps = env.get_steps(behaviorName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "bWUHqZOmPhZN",
    "outputId": "a6be9b47-1291-4506-8080-cc451a9f0a87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 243)\n"
     ]
    }
   ],
   "source": [
    "s = DecisionSteps.obs[0]\n",
    "print(s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "G6n67P9mPhZN",
    "outputId": "7b11fc89-eef3-468f-fae6-d6171e4a17cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Independent(Normal(loc: torch.Size([3, 39]), scale: torch.Size([3, 39])), 1)\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# getting the tuple (s, a, r, s', done)\n",
    "# ==================================================\n",
    "\n",
    "# action = param.act(states)\n",
    "# param.act calls sample_action_and_compute_log_pi(...) to calculate action\n",
    "\n",
    "# step-by-step run sample_action_and_compute_log_pi\n",
    "mu_given_s = agent.Normal(torch.FloatTensor(s).to(device)) \n",
    "print(mu_given_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "QByjJbxbPhZN",
    "outputId": "7064a640-825a-470f-d2f6-4b73374f1f96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 39])\n"
     ]
    }
   ],
   "source": [
    "u = mu_given_s.rsample() \n",
    "print(u.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "Z4blcXwuPhZN",
    "outputId": "fcd73f25-6856-45d1-f4d8-42bb4f56614e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 39])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tanh(u)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "tOGSM0UHPhZN",
    "outputId": "a439a6be-adb7-4169-bca7-49d8ecee2709"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-25.4253, -23.9051, -26.7177], device='cuda:0', grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "log_pi_a_given_s = mu_given_s.log_prob(u) - (2 * (np.log(2) - u - F.softplus(-2 * u))).sum(dim=1)\n",
    "print(log_pi_a_given_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "X8YY--l4PhZO",
    "outputId": "35c6eace-3df9-4566-ef21-9a222c4449ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 39)\n"
     ]
    }
   ],
   "source": [
    "print(a.detach().cpu().numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "tDRRdUu6PhZO",
    "outputId": "a4ecaed4-0a99-423b-cced-9c8387faa82e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 39)\n"
     ]
    }
   ],
   "source": [
    "# finish step-by-step run sample_action_and_compute_log_pi, try call the function directly\n",
    "a, _ = agent.sample_action_and_compute_log_pi(torch.FloatTensor(s).to(device), use_reparametrization_trick=False)\n",
    "a = a.detach().cpu().numpy()\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "TkWet4iQPhZO",
    "outputId": "5c906681-047d-440c-89cf-2d1c55faaa8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 39)\n"
     ]
    }
   ],
   "source": [
    "a = agent.act(torch.FloatTensor(s).to(device))\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlagents_envs.base_env import ActionTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlagents_envs.base_env.ActionTuple at 0x2058df47670>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ActionTuple(np.array([[1.0,0.0]], dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ActionTuple(np.array(a, dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.set_actions(behaviorName, a)\n",
    "env.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "EGYgB3lkPhZO",
    "outputId": "f599af51-cdfd-4065-f9fa-91d4741fa800"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 243) ,  (3, 1) ,  (3, 1)\n"
     ]
    }
   ],
   "source": [
    "# next_obs, reward, done, _ = env.step(action)\n",
    "NextDecisionSteps, NextTerminalSteps = env.get_steps(behaviorName)\n",
    "ns = NextDecisionSteps.obs[0]\n",
    "reward = NextDecisionSteps.reward\n",
    "reward = np.expand_dims(reward, axis=1)\n",
    "done = np.array([[0]]*N_AGENTS ) \n",
    "print(ns.shape, ', ', reward.shape, ', ', done.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "t8s8yTIsPhZO",
    "outputId": "55377eec-b6e2-4abf-d3aa-3ff1d224598b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]] \n",
      " 0.0\n"
     ]
    }
   ],
   "source": [
    "print(reward, \"\\n\", np.mean(reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlagents_envs.base_env.ActionTuple at 0x205b32f0ca0>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 39)\n"
     ]
    }
   ],
   "source": [
    "# from ActionTuple to np array\n",
    "print(a._continuous.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "gQkPFyLqPhZP"
   },
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# storing it to the buffer\n",
    "# ==================================================\n",
    "\n",
    "#buf.push(Transition(obs, action, reward, next_obs, done))\n",
    "buf.push(Transition(s, a._continuous, reward, ns, done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "45N21aUPPhZP",
    "outputId": "d8417739-8f6c-4492-b449-3ce23dde25ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 39) (3, 1)\n"
     ]
    }
   ],
   "source": [
    "print(buf.memory[0].a.shape, buf.memory[0].r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "V1C5eZyuPhZP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13  reset training!\n",
      "32  reset training!\n",
      "49  reset training!\n",
      "67  reset training!\n",
      "88  reset training!\n",
      "103  reset training!\n"
     ]
    }
   ],
   "source": [
    "# run a loop to fill the buffer with batch_size so we can update NN\n",
    "DecisionSteps, TerminalSteps = env.get_steps(behaviorName)\n",
    "for i in range(2*batch_size):\n",
    "    s = DecisionSteps.obs[0]\n",
    "    a = agent.act(torch.FloatTensor(s).to(device))\n",
    "    a = ActionTuple(np.array(a, dtype=np.float32))\n",
    "    env.set_actions(behaviorName, a)\n",
    "    env.step()\n",
    "    NextDecisionSteps, NextTerminalSteps = env.get_steps(behaviorName)\n",
    "    \n",
    "    #if next decision step misses some agents, then reset \n",
    "    if(len(NextDecisionSteps)!= N_AGENTS): \n",
    "        print(i, \" reset training!\")\n",
    "        env.reset()    \n",
    "        DecisionSteps, TerminalSteps = env.get_steps(behaviorName)\n",
    "    else: \n",
    "        ns = NextDecisionSteps.obs[0]\n",
    "        reward = NextDecisionSteps.reward\n",
    "        reward = np.expand_dims(reward, axis=1)\n",
    "        done = np.array([[0]]*N_AGENTS ) \n",
    "        buf.push(Transition(s, a._continuous, reward, ns, done))\n",
    "        DecisionSteps, TerminalSteps = NextDecisionSteps, NextTerminalSteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "Vu8-8j8PPhZP",
    "outputId": "f3f94228-85da-4c89-96b8-76c84d48314e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(buf.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "qODuyE2UPhZP",
    "outputId": "17f04f39-2c22-46bd-febd-d6fad2ea547b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 3 64\n",
      "(3, 243) (3, 39) (3, 1) (3, 243) (3, 1)\n"
     ]
    }
   ],
   "source": [
    "# agent.update_networks(buf.sample(batch_size))\n",
    "\n",
    "#step by step to run update_network\n",
    "\n",
    "# buf.sample(batch_size)\n",
    "#Transition = namedtuple('Transition', 's a r ns d')\n",
    "#Batch = namedtuple('Batch', 's a r ns d')\n",
    "experiences = random.sample(buf.memory, batch_size) \n",
    "print(batch_size, N_AGENTS, len(experiences))\n",
    "print(experiences[0].s.shape, experiences[0].a.shape, experiences[0].r.shape, \\\n",
    "      experiences[0].ns.shape, experiences[0].d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "umUqxUoDPhZQ",
    "outputId": "231b672e-699d-4efb-9e3f-973577ed260e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192, 243)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack([e.s for e in experiences if e is not None]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "peyMCxoMPhZQ",
    "outputId": "24994168-e86d-4ee3-e5e7-1eaef4e21024"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(192, 243) (192, 39) (192, 1) (192, 243) (192, 1)\n"
     ]
    }
   ],
   "source": [
    "#Find all states in memory batch, append vertically, convert to torch with float values and send to CPU or GPU\n",
    "bs  = np.vstack([e.s for e in experiences if e is not None])\n",
    "ba  = np.vstack([e.a for e in experiences if e is not None])\n",
    "br  = np.vstack([e.r for e in experiences if e is not None])\n",
    "b_ns = np.vstack([e.ns for e in experiences if e is not None])\n",
    "bd  = np.vstack([e.d for e in experiences if e is not None])\n",
    "print(bs.shape, ba.shape, br.shape, b_ns.shape, bd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "XW2KmgMTPhZQ",
    "outputId": "c4f9a777-c3ae-4e6f-d395-167b912009f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([192, 243])\n"
     ]
    }
   ],
   "source": [
    "bs  = torch.from_numpy(np.vstack([e.s for e in experiences if e is not None])).float().to(device)\n",
    "print(bs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "12oUx3ETPhZQ",
    "outputId": "4b53727f-0334-495c-d8b6-bfe156621620"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([192, 243]) torch.Size([192, 39]) torch.Size([192, 1]) torch.Size([192, 243]) torch.Size([192, 1])\n"
     ]
    }
   ],
   "source": [
    "bs  = torch.from_numpy(np.vstack([e.s for e in experiences if e is not None])).float().to(device)\n",
    "ba  = torch.from_numpy(np.vstack([e.a for e in experiences if e is not None])).float().to(device)\n",
    "br  = torch.from_numpy(np.vstack([e.r for e in experiences if e is not None])).float().to(device)\n",
    "b_ns  = torch.from_numpy(np.vstack([e.ns for e in experiences if e is not None])).float().to(device)\n",
    "bd  = torch.from_numpy(np.vstack([e.d for e in experiences if e is not None])).float().to(device)\n",
    "print(bs.shape, ba.shape, br.shape, b_ns.shape, bd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "yvC1FumMPhZQ",
    "outputId": "1b984d1a-7106-4f5c-f7fa-b482a7144848"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([192, 243]) torch.Size([192, 39]) torch.Size([192, 1]) torch.Size([192, 243]) torch.Size([192, 1])\n"
     ]
    }
   ],
   "source": [
    "# finish step-by-step test of buf.sample\n",
    "# directly call buf.sample(batch_size)\n",
    "batch = buf.sample(batch_size)\n",
    "print(batch.s.shape, batch.a.shape, batch.r.shape, batch.ns.shape, batch.d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "VaY_bou9PhZQ"
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Step 12: calculating targets\n",
    "# ========================================\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    na, log_pi_na_given_ns = agent.sample_action_and_compute_log_pi(b_ns, use_reparametrization_trick=False)\n",
    "    targets = br + agent.gamma * (1 - bd) * \\\n",
    "              (agent.min_i_12(agent.Q1_targ(b_ns, na), agent.Q2_targ(b_ns, na)) - agent.alpha * log_pi_na_given_ns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "h_WUJIdFPhZQ"
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Step 13: learning the Q functions\n",
    "# ========================================\n",
    "\n",
    "Q1_predictions = agent.Q1(bs, ba)\n",
    "Q1_loss = torch.mean((Q1_predictions - targets) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "DRqn5KpmPhZR",
    "outputId": "9025228e-4ce3-46a7-e5d1-9ff2203d787b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.9385, device='cuda:0', grad_fn=<MeanBackward0>) \n",
      " 5.938521862030029\n"
     ]
    }
   ],
   "source": [
    "print(Q1_loss, \"\\n\", float(Q1_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "pkGXh-stPhZR"
   },
   "outputs": [],
   "source": [
    "agent.Q1_optimizer.zero_grad()\n",
    "Q1_loss.backward()\n",
    "agent.clip_gradient(net=agent.Q1)\n",
    "agent.Q1_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "id": "Hm4-2TuePhZR"
   },
   "outputs": [],
   "source": [
    "Q2_predictions = agent.Q2(bs, ba)\n",
    "Q2_loss = torch.mean((Q2_predictions - targets) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "id": "PJlzhZLSPhZT",
    "outputId": "2d4bb012-8527-4372-8fda-c772b889f951"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.8861, device='cuda:0', grad_fn=<MeanBackward0>) \n",
      " 6.886084079742432\n"
     ]
    }
   ],
   "source": [
    "print(Q2_loss, \"\\n\", float(Q2_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "pRO2Jb2NPhZT"
   },
   "outputs": [],
   "source": [
    "agent.Q2_optimizer.zero_grad()\n",
    "Q2_loss.backward()\n",
    "agent.clip_gradient(net=agent.Q2)\n",
    "agent.Q2_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "d-r5x0XIPhZU"
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Step 14: learning the policy\n",
    "# ========================================\n",
    "\n",
    "for param in agent.Q1.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in agent.Q2.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "id": "9GCfP2KkPhZU"
   },
   "outputs": [],
   "source": [
    "a, log_pi_a_given_s = agent.sample_action_and_compute_log_pi(bs, use_reparametrization_trick=True)\n",
    "policy_loss = - torch.mean(agent.min_i_12(agent.Q1(bs, a), agent.Q2(bs, a)) - agent.alpha * log_pi_a_given_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "8-H-tjI2PhZU",
    "outputId": "cfec59ac-2176-4c66-844e-67a7be1520e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.5262, device='cuda:0', grad_fn=<NegBackward0>) \n",
      " -2.526196002960205\n"
     ]
    }
   ],
   "source": [
    "print(policy_loss, \"\\n\", float(policy_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "mY8HmUW0PhZU"
   },
   "outputs": [],
   "source": [
    "agent.Normal_optimizer.zero_grad()\n",
    "policy_loss.backward()\n",
    "agent.clip_gradient(net=agent.Normal)\n",
    "agent.Normal_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "NTjTCSpBPhZU"
   },
   "outputs": [],
   "source": [
    "for param in agent.Q1.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in agent.Q2.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "mYBNyJ5mPhZU"
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Step 15: update target networks\n",
    "# ========================================\n",
    "\n",
    "with torch.no_grad():\n",
    "    agent.polyak_update(old_net=agent.Q1_targ, new_net=agent.Q1)\n",
    "    agent.polyak_update(old_net=agent.Q2_targ, new_net=agent.Q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "cSMaX5BYPhZU"
   },
   "outputs": [],
   "source": [
    "# finish step-by-step test of agent.update_network\n",
    "# directly call agent.update_networks(buf.sample(batch_size))\n",
    "agent.update_networks(buf.sample(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "id": "11TDLQ_OPhZV"
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MKptuAzHPhZV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "3. NN with policy interacts with 3D Ball to collect training data (MLAgent 10).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
