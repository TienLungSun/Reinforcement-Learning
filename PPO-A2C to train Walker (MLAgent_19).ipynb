{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: RL-Adventure https://github.com/higgsfield/RL-Adventure-2/blob/master/3.ppo.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "z29hg89Qmg-S"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.base_env import ActionTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "U5TAt7Mcmg-W",
    "outputId": "6ef7273c-d02b-45b2-9512-a2e57da25817"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda GeForce GTX 1660 SUPER\n"
     ]
    }
   ],
   "source": [
    "if(torch.cuda.is_available()):\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(device, torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device= torch.device(\"cpu\")\n",
    "    print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor and critic NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (variales in CAPTICAL are global variables)\n",
    "N_STATES  = 243\n",
    "N_ACTIONS = 39\n",
    "HIDDEN_UNITS = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bEjeAPtXmg-Y"
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
    "        nn.init.constant_(m.bias, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9MJKXhTgmg-Y"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(N_STATES, HIDDEN_UNITS),\n",
    "            nn.LayerNorm(HIDDEN_UNITS),\n",
    "            nn.Linear(HIDDEN_UNITS, HIDDEN_UNITS),\n",
    "            nn.LayerNorm(HIDDEN_UNITS),\n",
    "            nn.Linear(HIDDEN_UNITS, 1)\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(N_STATES, HIDDEN_UNITS),\n",
    "            nn.LayerNorm(HIDDEN_UNITS),\n",
    "            nn.Linear(HIDDEN_UNITS, HIDDEN_UNITS),\n",
    "            nn.LayerNorm(HIDDEN_UNITS),\n",
    "            nn.Linear(HIDDEN_UNITS, N_ACTIONS)\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.ones(1, N_ACTIONS) * 0.0)\n",
    "        self.apply(init_weights)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        value = self.critic(x)\n",
    "        mu    = self.actor(x)\n",
    "        std   = self.log_std.exp().expand_as(mu)\n",
    "        dist  = Normal(mu, std)\n",
    "        return dist, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "sj7O_oOUmg-Z"
   },
   "outputs": [],
   "source": [
    "NET = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0003\n",
    "OPTIMIZER = optim.Adam(NET.parameters(), lr=LEARNING_RATE )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: Actor-Critics NN interacting with Unity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = UnityEnvironment(file_name= None, base_port=5004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV.reset()\n",
    "behavior_names = list(ENV.behavior_specs.keys())\n",
    "BEHAVIOR_NAME = behavior_names[0]\n",
    "print(BEHAVIOR_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionSteps, TerminalSteps = ENV.get_steps(BEHAVIOR_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = DecisionSteps.obs[0]\n",
    "print(s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = torch.FloatTensor(s)       \n",
    "dist, value = NET(s.to(device))\n",
    "print(dist, \"\\n\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dist.sample() \n",
    "log_prob = dist.log_prob(a)\n",
    "print(a, \"\\n\", log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.cpu().detach().numpy()\n",
    "a = ActionTuple(np.array(a, dtype=np.float32))\n",
    "ENV.set_actions(BEHAVIOR_NAME, a)   \n",
    "ENV.step()\n",
    "a = a._continuous #convert from ActionTuple to np.array\n",
    "a = torch.FloatTensor(a) # convert from np.array to Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interacts with Unity to collect training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Interact_with_Unity_one_step (DecisionSteps):\n",
    "    # ENV and NET are global variables        \n",
    "    s = DecisionSteps.obs[0]  \n",
    "    s = torch.FloatTensor(s)       \n",
    "    dist, value = NET(s.to(device))\n",
    "    a = dist.sample() \n",
    "    log_prob = dist.log_prob(a)\n",
    "    \n",
    "    a = a.cpu().detach().numpy()\n",
    "    a = ActionTuple(np.array(a, dtype=np.float32))\n",
    "    ENV.set_actions(BEHAVIOR_NAME, a)   \n",
    "    ENV.step()\n",
    "    a = a._continuous #convert from ActionTuple to np.array\n",
    "    a = torch.FloatTensor(a) # convert from np.array to Tensor\n",
    "    return s, value, a, log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Collect_REWARDS_and_MASKS (step, AgentSteps, flag): \n",
    "    #flag=1:decision, 0: terminal steps\n",
    "    #REWARDS, MASKS, NEXT_STATES are gloable variables\n",
    "    r = AgentSteps.reward\n",
    "    r = torch.FloatTensor(r).unsqueeze(1)\n",
    "    s = torch.FloatTensor(AgentSteps.obs[0])\n",
    "    s = torch.FloatTensor(s).to(device) \n",
    "    for idx in range(len(AgentSteps)):\n",
    "        AgentID = AgentSteps.agent_id[idx]\n",
    "        REWARDS[step][AgentID]=r[idx]\n",
    "        MASKS[step][AgentID]= flag\n",
    "        NEXT_STATES[step][AgentID]=s[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_training_data (print_message):  \n",
    "    # Interact with Unity INTERACTION_STEPS to collect training data\n",
    "    # The total number of training data collected (buffer size) is INTERACTION_STEPS*N_AGENTS \n",
    "    #ENV, BEHAVIOR_NAME are gloabl variables\n",
    "    #STATES, ACTIONS, LOG_PROBS, VALUES, REWARDS, MASKS, NEXT_STATES are global variables (tensor array)\n",
    "    if(print_message):\n",
    "        print(\"Collecting \", INTERACTION_STEPS, \" training steps from \", N_AGENTS, \" agents\", end=\": \")\n",
    "    step = 0  \n",
    "    DecisionSteps, TerminalSteps = ENV.get_steps(BEHAVIOR_NAME)\n",
    "    while(step < INTERACTION_STEPS): #try to run TIME_HORIZON good steps\n",
    "        #if we have no decision agents,then continue next loop without increase step\n",
    "        if(len(DecisionSteps) == 0): \n",
    "            ENV.reset() \n",
    "            DecisionSteps, TerminalSteps = ENV.get_steps(BEHAVIOR_NAME)\n",
    "            continue #continue next while loop without increase step\n",
    "        \n",
    "        # Interacts with Unity one step\n",
    "        s, value, a, log_prob = Interact_with_Unity_one_step (DecisionSteps)\n",
    "        NextDecisionSteps, NextTerminalSteps = ENV.get_steps(BEHAVIOR_NAME)\n",
    "\n",
    "        #if this or next decision step misses some agents, then do not collect data\n",
    "        if(len(DecisionSteps)!= N_AGENTS or len(NextDecisionSteps)!= N_AGENTS):\n",
    "            DecisionSteps, TerminalSteps = NextDecisionSteps, NextTerminalSteps\n",
    "            continue      #continue next while loop without increase step\n",
    "        \n",
    "        #else this and next decision steps includes all agents, collect (s, a, r, s1)\n",
    "        for idx in range(len(DecisionSteps)):\n",
    "            #find decision agents and record their state, value and ACTIONS\n",
    "            AgentID = DecisionSteps.agent_id[idx]\n",
    "            STATES[step][AgentID]=s[idx]\n",
    "            VALUES[step][AgentID]=value[idx]\n",
    "            ACTIONS[step][AgentID]=a[idx]\n",
    "            LOG_PROBS[step][AgentID]=log_prob[idx]\n",
    "\n",
    "        #collect reward of this action from next decision and terminal steps\n",
    "        if(len(NextTerminalSteps) >0):\n",
    "            #if next step has terminal agents, then collect terminal agents first\n",
    "            Collect_REWARDS_and_MASKS(step, NextTerminalSteps, 0)\n",
    "        else:  #else collect r and next state from decision steps\n",
    "            Collect_REWARDS_and_MASKS(step, NextDecisionSteps, 1)\n",
    "        \n",
    "        if(print_message and (step % 500)==0):\n",
    "            print(step, end=\",\")\n",
    "        step = step + 1\n",
    "        DecisionSteps, TerminalSteps = NextDecisionSteps, NextTerminalSteps\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(next_value):\n",
    "    value1 = VALUES + [next_value.cpu()]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(INTERACTION_STEPS )):\n",
    "        delta = REWARDS[step] + GAMMA*value1[step + 1]*MASKS[step]-value1[step]\n",
    "        gae = delta + GAMMA*LAMBD*MASKS[step]*gae\n",
    "        returns.insert(0, gae + VALUES[step])\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test interaction with Unity to collect training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = UnityEnvironment(file_name= None, base_port=5004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walker?team=0\n"
     ]
    }
   ],
   "source": [
    "ENV.reset()\n",
    "behavior_names = list(ENV.behavior_specs.keys())\n",
    "BEHAVIOR_NAME = behavior_names[0]\n",
    "print(BEHAVIOR_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor arrays to store training data (s,a,r,s1) from N_AGENTS performing INTERACTION_STEPS\n",
    "N_AGENTS = 10  #The number of training scenes in Unity \n",
    "\n",
    "INTERACTION_STEPS = 254  #(Walker.yaml 2048) buffer_size=N_AGENTS * INTERACTION_STEPS=20480\n",
    "\n",
    "a = torch.FloatTensor([[0]]*N_AGENTS ) \n",
    "b = torch.FloatTensor([[0]*N_ACTIONS]*N_AGENTS ) \n",
    "c = torch.FloatTensor([[0]*N_STATES]*N_AGENTS ) \n",
    "\n",
    "VALUES =REWARDS = MASKS = [a]*INTERACTION_STEPS\n",
    "LOG_PROBS = ACTIONS = [b]*INTERACTION_STEPS\n",
    "STATES = NEXT_STATES = [c]*INTERACTION_STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting  254  training steps from  10  agents: 0,\n"
     ]
    }
   ],
   "source": [
    "collect_training_data(print_message=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254 torch.Size([10, 39])\n",
      "254 torch.Size([10, 1])\n",
      "254 torch.Size([10, 1])\n",
      "254 torch.Size([10, 1])\n",
      "254 torch.Size([10, 243])\n",
      "254 torch.Size([10, 39])\n",
      "254 torch.Size([10, 243])\n"
     ]
    }
   ],
   "source": [
    "print(len(LOG_PROBS), LOG_PROBS[0].shape)\n",
    "print(len(VALUES), VALUES[0].shape)\n",
    "print(len(REWARDS), REWARDS[0].shape)\n",
    "print(len(MASKS), MASKS[0].shape)\n",
    "print(len(STATES), STATES[0].shape)\n",
    "print(len(ACTIONS), ACTIONS[0].shape)\n",
    "print(len(NEXT_STATES), NEXT_STATES[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send last next state to calculate value\n",
    "_, next_value = NET(NEXT_STATES[-1].to(device)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.995\n",
    "LAMBD = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "RETURNS = compute_gae(next_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "MERGED_RETURNS   = torch.cat(RETURNS).detach()\n",
    "MERGED_LOG_PROBS = torch.cat(LOG_PROBS).detach()\n",
    "MERGED_VALUES    = torch.cat(VALUES).detach()\n",
    "MERGED_STATES    = torch.cat(STATES) \n",
    "MERGED_NEXT_STATES   = torch.cat(NEXT_STATES) \n",
    "MERGED_ACTIONS   = torch.cat(ACTIONS)\n",
    "MERGED_ADVANTAGES = MERGED_RETURNS - MERGED_VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2540 torch.Size([1])\n",
      "2540 torch.Size([39])\n",
      "2540 torch.Size([1])\n",
      "2540 torch.Size([243])\n",
      "2540 torch.Size([243])\n",
      "2540 torch.Size([39])\n",
      "2540 torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "print(len(MERGED_RETURNS), MERGED_RETURNS[0].shape)\n",
    "print(len(MERGED_LOG_PROBS), MERGED_LOG_PROBS[0].shape)\n",
    "print(len(MERGED_VALUES), MERGED_VALUES[0].shape)\n",
    "print(len(MERGED_STATES), MERGED_STATES[0].shape)\n",
    "print(len(MERGED_NEXT_STATES), MERGED_NEXT_STATES[0].shape)\n",
    "print(len(MERGED_ACTIONS), MERGED_ACTIONS[0].shape)\n",
    "print(len(MERGED_ADVANTAGES), MERGED_ADVANTAGES[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.983325958251953"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(torch.mean(MERGED_RETURNS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_iter():\n",
    "    buffer_size = MERGED_STATES.size(0)\n",
    "    for _ in range(buffer_size// BATCH_SIZE ):\n",
    "        rand_ids = np.random.randint(0, buffer_size, BATCH_SIZE )\n",
    "        yield MERGED_STATES[rand_ids, :], MERGED_ACTIONS[rand_ids, :], MERGED_NEXT_STATES[rand_ids, :],\\\n",
    "              MERGED_LOG_PROBS[rand_ids, :], MERGED_RETURNS[rand_ids, :], MERGED_ADVANTAGES[rand_ids, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_update():\n",
    "    #print(\"epoch:\")\n",
    "    for epoch in range(N_EPOCH):\n",
    "        #print(epoch, end = \", \")\n",
    "        for b_s, b_a, b_s_, b_old_LOG_PROBS, b_return, b_advantage in ppo_iter():\n",
    "            dist, value = NET(b_s.to(device))       \n",
    "            critic_loss = (b_return.to(device) - value).pow(2).mean()\n",
    "            entropy = dist.entropy().mean()\n",
    "            b_a_new = dist.sample()\n",
    "            b_new_LOG_PROBS = dist.log_prob(b_a_new)\n",
    "            ratio = (b_new_LOG_PROBS - b_old_LOG_PROBS.to(device)).exp()\n",
    "            surr1 = ratio * b_advantage.to(device)\n",
    "            surr2 = torch.clamp(ratio, 1.0-EPSILON, 1.0+EPSILON) * b_advantage.to(device)\n",
    "            actor_loss  = - torch.min(surr1, surr2).mean()\n",
    "            loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy\n",
    "            OPTIMIZER.zero_grad()\n",
    "            loss.backward()\n",
    "            OPTIMIZER.step()\n",
    "    return float(critic_loss), float(actor_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try one training iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x2GNW0jsmg-Z",
    "outputId": "5a9937db-f4e6-4ff5-f4f0-35ebedb68d61"
   },
   "outputs": [],
   "source": [
    "ENV = UnityEnvironment(file_name= None, base_port=5004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_SeNL04Fmg-b"
   },
   "outputs": [],
   "source": [
    "ENV.reset()\n",
    "behavior_names = list(ENV.behavior_specs.keys())\n",
    "BEHAVIOR_NAME = behavior_names[0]\n",
    "print(BEHAVIOR_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_training_data(print_message=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send last next state to calculate value\n",
    "_, next_value = NET(NEXT_STATES[-1].to(device)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RETURNS = compute_gae(next_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MERGED_RETURNS   = torch.cat(RETURNS).detach()\n",
    "MERGED_LOG_PROBS = torch.cat(LOG_PROBS).detach()\n",
    "MERGED_VALUES    = torch.cat(VALUES).detach()\n",
    "MERGED_STATES    = torch.cat(STATES) \n",
    "MERGED_NEXT_STATES   = torch.cat(NEXT_STATES) \n",
    "MERGED_ACTIONS   = torch.cat(ACTIONS)\n",
    "MERGED_ADVANTAGES = MERGED_RETURNS - MERGED_VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 254         #Walker.yaml 2048\n",
    "BETA = 0.005\n",
    "EPSILON = 0.2\n",
    "N_EPOCH = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_loss, actor_loss = ppo_update()\n",
    "print(critic_loss, actor_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interact with Unity for N steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_STEPS = 30000    #Walker.yaml 30M\n",
    "SUMMARY_FREQ = 3000  #Walker.yaml 30K\n",
    "TIME_HORIZON = 1000  #I do not use this parameter in my porgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = UnityEnvironment(file_name= None, base_port=5004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV.reset()\n",
    "BEHAVIOR_NAME = list(ENV.behavior_specs.keys())\n",
    "BEHAVIOR_NAME = BEHAVIOR_NAME[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RewardLst = []\n",
    "ActorLossLst = []\n",
    "CriticLossLst = []\n",
    "steps  = 0 \n",
    "summary = SUMMARY_FREQ\n",
    "\n",
    "while (steps < MAX_STEPS):\n",
    "    #print(\"Collecting \", INTERACTION_STEPS, \" training steps from \", N_AGENTS, \" agents\", end=\": \")\n",
    "    collect_training_data(print_message=True)\n",
    "    _, next_value = NET(NEXT_STATES[-1].to(device)) \n",
    "    \n",
    "    print(\"Compute GAE of these training data set\")\n",
    "    RETURNS = compute_gae(next_value)\n",
    "    MERGED_RETURNS   = torch.cat(RETURNS).detach()\n",
    "    MERGED_LOG_PROBS = torch.cat(LOG_PROBS).detach()\n",
    "    MERGED_VALUES    = torch.cat(VALUES).detach()\n",
    "    MERGED_STATES    = torch.cat(STATES) \n",
    "    MERGED_NEXT_STATES    = torch.cat(NEXT_STATES) \n",
    "    MERGED_ACTIONS   = torch.cat(ACTIONS)\n",
    "    MERGED_ADVANTAGES = MERGED_RETURNS - MERGED_VALUES\n",
    "    \n",
    "    print(\"Optimize NN with PPO\")\n",
    "    critic_loss, actor_loss = ppo_update()\n",
    "    if(steps > summary):\n",
    "        print(\"No of training steps = \", steps)\n",
    "        mean_reward = float(torch.mean(MERGED_RETURNS))\n",
    "        print(\"Reward = \", round(mean_reward, 2))\n",
    "        print(\"Critic loss = \", round(critic_loss, 2), \" Actor loss = \", round(actor_loss, 2))\n",
    "        RewardLst.append(mean_reward)\n",
    "        CriticLossLst.append(critic_loss)\n",
    "        ActorLossLst.append(actor_loss)\n",
    "        summary += SUMMARY_FREQ\n",
    "\n",
    "    steps += INTERACTION_STEPS*N_AGENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(RewardLst)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(18, 6))\n",
    "fig.add_subplot(1, 2, 1)  # 1 row, 2 columns\n",
    "plt.plot(CriticLossLst)\n",
    "\n",
    "fig.add_subplot(1, 2, 2) \n",
    "plt.plot(ActorLossLst)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "3. NN with policy interacts with 3D Ball to collect training data (MLAgent 10).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
