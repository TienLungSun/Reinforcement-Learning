{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Reference: https://github.com/zhihanyang2022/pytorch-sac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "z29hg89Qmg-S"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal, Independent\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from mlagents_envs.environment import UnityEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "U5TAt7Mcmg-W",
    "outputId": "6ef7273c-d02b-45b2-9512-a2e57da25817"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "if(torch.cuda.is_available()):\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(device, torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device= torch.device(\"cpu\")\n",
    "    print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "\n",
    "Transition = namedtuple('Transition', 's a r ns d')\n",
    "Batch = namedtuple('Batch', 's a r ns d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User(name='tester', age='22', id='464643123')\n"
     ]
    }
   ],
   "source": [
    "# example of namedtuple\n",
    "User = namedtuple('User', 'name age id')\n",
    "user = User('tester', '22', '464643123')\n",
    "print(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, capacity: int):\n",
    "        self.capacity = capacity\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, transition: Transition) -> None:\n",
    "        self.memory.appendleft(transition)\n",
    "\n",
    "    def ready_for(self, batch_size: int) -> bool:\n",
    "        if len(self.memory) >= batch_size:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def sample(self, batch_size: int) -> Batch:\n",
    "        experiences = random.sample(self.memory, batch_size)\n",
    "        s  = torch.from_numpy(np.vstack([e.s for e in experiences if e is not None])).float().to(device)\n",
    "        a  = torch.from_numpy(np.vstack([e.a for e in experiences if e is not None])).float().to(device)\n",
    "        r  = torch.from_numpy(np.vstack([e.r for e in experiences if e is not None])).float().to(device)\n",
    "        ns  = torch.from_numpy(np.vstack([e.ns for e in experiences if e is not None])).float().to(device)\n",
    "        d  = torch.from_numpy(np.vstack([e.d for e in experiences if e is not None])).float().to(device)\n",
    "        return Batch(s, a, r, ns, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3\n"
     ]
    }
   ],
   "source": [
    "# test buffer\n",
    "b = ReplayBuffer(capacity=5)  \n",
    "b.push(Transition(1,2, 3, 4, 5))\n",
    "print(b.memory[0].a, b.memory[0].r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_net(\n",
    "        num_in:int,\n",
    "        num_out:int,\n",
    "        final_activation,  # e.g. nn.Tanh\n",
    "        num_hidden_layers:int=5,\n",
    "        num_neurons_per_hidden_layer:int=64\n",
    "    ) -> nn.Sequential:\n",
    "\n",
    "    layers = []\n",
    "\n",
    "    layers.extend([\n",
    "        nn.Linear(num_in, num_neurons_per_hidden_layer),\n",
    "        nn.ReLU(),\n",
    "    ])\n",
    "\n",
    "    for _ in range(num_hidden_layers):\n",
    "        layers.extend([\n",
    "            nn.Linear(num_neurons_per_hidden_layer, num_neurons_per_hidden_layer),\n",
    "            nn.ReLU(),\n",
    "        ])\n",
    "\n",
    "    layers.append(nn.Linear(num_neurons_per_hidden_layer, num_out))\n",
    "\n",
    "    if final_activation is not None:\n",
    "        layers.append(final_activation)\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalPolicyNet(nn.Module):\n",
    "\n",
    "    \"\"\"Outputs a distribution with parameters learnable by gradient descent.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        super(NormalPolicyNet, self).__init__()\n",
    "        self.shared_net   = get_net(num_in=input_dim, num_out=64, final_activation=nn.ReLU())\n",
    "        self.means_net    = nn.Linear(64, action_dim)\n",
    "        self.log_stds_net = nn.Linear(64, action_dim)\n",
    "\n",
    "    def forward(self, states: torch.tensor):\n",
    "\n",
    "        out = self.shared_net(states)\n",
    "        means, log_stds = self.means_net(out), self.log_stds_net(out)\n",
    "\n",
    "        # the gradient of computing log_stds first and then using torch.exp\n",
    "        # is much more well-behaved then computing stds directly using nn.Softplus()\n",
    "        # ref: https://github.com/openai/spinningup/blob/master/spinup/algos/pytorch/sac/core.py#L26\n",
    "\n",
    "        LOG_STD_MAX = 2\n",
    "        LOG_STD_MIN = -20\n",
    "\n",
    "        stds = torch.exp(torch.clamp(log_stds, LOG_STD_MIN, LOG_STD_MAX))\n",
    "\n",
    "        return Independent(Normal(loc=means, scale=stds), reinterpreted_batch_ndims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet(nn.Module):\n",
    "\n",
    "    \"\"\"Has little quirks; just a wrapper so that I don't need to call concat many times\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        super(QNet, self).__init__()\n",
    "        self.net = get_net(num_in=input_dim+action_dim, num_out=1, final_activation=None)\n",
    "\n",
    "    def forward(self, states: torch.tensor, actions: torch.tensor):\n",
    "        return self.net(torch.cat([states, actions], dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "\n",
    "        self.Normal   = NormalPolicyNet(input_dim=input_dim, action_dim=action_dim).to(device)\n",
    "        self.Normal_optimizer = optim.Adam(self.Normal.parameters(), lr=1e-3)\n",
    "\n",
    "        self.Q1       = QNet(input_dim=input_dim, action_dim=action_dim).to(device)\n",
    "        self.Q1_targ  = QNet(input_dim=input_dim, action_dim=action_dim).to(device)\n",
    "        self.Q1_targ.load_state_dict(self.Q1.state_dict())\n",
    "        self.Q1_optimizer = optim.Adam(self.Q1.parameters(), lr=1e-3)\n",
    "\n",
    "        self.Q2       = QNet(input_dim=input_dim, action_dim=action_dim).to(device)\n",
    "        self.Q2_targ  = QNet(input_dim=input_dim, action_dim=action_dim).to(device)\n",
    "        self.Q2_targ.load_state_dict(self.Q2.state_dict())\n",
    "        self.Q2_optimizer = optim.Adam(self.Q2.parameters(), lr=1e-3)\n",
    "\n",
    "        self.gamma = 0.99\n",
    "        self.alpha = 0.1\n",
    "        self.polyak = 0.995\n",
    "\n",
    "    # ==================================================================================================================\n",
    "    # Helper methods (it is generally not my style of using helper methods but here they improve readability)\n",
    "    # ==================================================================================================================\n",
    "\n",
    "    def min_i_12(self, a: torch.tensor, b: torch.tensor) -> torch.tensor:\n",
    "        return torch.min(a, b)\n",
    "\n",
    "    def sample_action_and_compute_log_pi(self, state, use_reparametrization_trick):\n",
    "        mu_given_s = self.Normal(state)  # in paper, mu represents the normal distribution\n",
    "        # in paper, u represents the un-squashed action; nu stands for next u's\n",
    "        # actually, we can just use reparametrization trick in both Step 12 and 14, but it might be good to separate\n",
    "        # the two cases for pedagogical purposes, i.e., using reparametrization trick is a must in Step 14\n",
    "        u = mu_given_s.rsample() if use_reparametrization_trick else mu_given_s.sample()\n",
    "        a = torch.tanh(u)\n",
    "        # the following line of code is not numerically stable:\n",
    "        # log_pi_a_given_s = mu_given_s.log_prob(u) - torch.sum(torch.log(1 - torch.tanh(u) ** 2), dim=1)\n",
    "        # ref: https://github.com/vitchyr/rlkit/blob/0073d73235d7b4265cd9abe1683b30786d863ffe/rlkit/torch/distributions.py#L358\n",
    "        # ref: https://github.com/tensorflow/probability/blob/master/tensorflow_probability/python/bijectors/tanh.py#L73\n",
    "        log_pi_a_given_s = mu_given_s.log_prob(u) - (2 * (np.log(2) - u - F.softplus(-2 * u))).sum(dim=1)\n",
    "        return a, log_pi_a_given_s\n",
    "\n",
    "    def clip_gradient(self, net: nn.Module) -> None:\n",
    "        for param in net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "\n",
    "    def polyak_update(self, old_net: nn.Module, new_net: nn.Module) -> None:\n",
    "        for old_param, new_param in zip(old_net.parameters(), new_net.parameters()):\n",
    "            old_param.data.copy_(old_param.data * self.polyak + new_param.data * (1 - self.polyak))\n",
    "\n",
    "    # ==================================================================================================================\n",
    "    # Methods for learning\n",
    "    # ==================================================================================================================\n",
    "\n",
    "    def update_networks(self, b: Batch) -> None:\n",
    "\n",
    "        # ========================================\n",
    "        # Step 12: calculating targets\n",
    "        # ========================================\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            na, log_pi_na_given_ns = self.sample_action_and_compute_log_pi(b.ns, use_reparametrization_trick=False)\n",
    "            targets = b.r + self.gamma * (1 - b.d) * \\\n",
    "                      (self.min_i_12(self.Q1_targ(b.ns, na), self.Q2_targ(b.ns, na)) - self.alpha * log_pi_na_given_ns)\n",
    "\n",
    "        # ========================================\n",
    "        # Step 13: learning the Q functions\n",
    "        # ========================================\n",
    "\n",
    "        Q1_predictions = self.Q1(b.s, b.a)\n",
    "        Q1_loss = torch.mean((Q1_predictions - targets) ** 2)\n",
    "\n",
    "        self.Q1_optimizer.zero_grad()\n",
    "        Q1_loss.backward()\n",
    "        self.clip_gradient(net=self.Q1)\n",
    "        self.Q1_optimizer.step()\n",
    "\n",
    "        Q2_predictions = self.Q2(b.s, b.a)\n",
    "        Q2_loss = torch.mean((Q2_predictions - targets) ** 2)\n",
    "\n",
    "        self.Q2_optimizer.zero_grad()\n",
    "        Q2_loss.backward()\n",
    "        self.clip_gradient(net=self.Q2)\n",
    "        self.Q2_optimizer.step()\n",
    "\n",
    "        # ========================================\n",
    "        # Step 14: learning the policy\n",
    "        # ========================================\n",
    "\n",
    "        for param in self.Q1.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.Q2.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        a, log_pi_a_given_s = self.sample_action_and_compute_log_pi(b.s, use_reparametrization_trick=True)\n",
    "        policy_loss = - torch.mean(self.min_i_12(self.Q1(b.s, a), self.Q2(b.s, a)) - self.alpha * log_pi_a_given_s)\n",
    "\n",
    "        self.Normal_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.clip_gradient(net=self.Normal)\n",
    "        self.Normal_optimizer.step()\n",
    "\n",
    "        for param in self.Q1.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.Q2.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        # ========================================\n",
    "        # Step 15: update target networks\n",
    "        # ========================================\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.polyak_update(old_net=self.Q1_targ, new_net=self.Q1)\n",
    "            self.polyak_update(old_net=self.Q2_targ, new_net=self.Q2)\n",
    "\n",
    "    def act(self, state)-> np.array:\n",
    "        # state: torch.FloatTensor(s).to(device)\n",
    "        action, _ = self.sample_action_and_compute_log_pi(state, use_reparametrization_trick=False)\n",
    "        return action.detach().cpu().numpy()  # no need to detach first because we are not using the reparametrization trick\n",
    "\n",
    "    def save_actor(self, save_dir: str, filename: str) -> None:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        torch.save(self.Normal.state_dict(), os.path.join(save_dir, filename))\n",
    "\n",
    "    def load_actor(self, save_dir: str, filename: str) -> None:\n",
    "        self.Normal.load_state_dict(torch.load(os.path.join(save_dir, filename)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instinate NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STATES  = 8\n",
    "N_ACTIONS =2\n",
    "\n",
    "N_AGENTS = 3  \n",
    "hidden_units = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "buf = ReplayBuffer(capacity=int(1e3))   #200000 in 3DBall.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    input_dim=N_STATES,\n",
    "    action_dim=N_ACTIONS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4 #64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-by-step test of the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name= None, base_port=5004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3DBall?team=0\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "behaviorNames = list(env.behavior_specs.keys())\n",
    "behaviorName = behaviorNames[0]\n",
    "print(behaviorName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionSteps, TerminalSteps = env.get_steps(behaviorName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 8)\n"
     ]
    }
   ],
   "source": [
    "s = DecisionSteps.obs[0]\n",
    "print(s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Independent(Normal(loc: torch.Size([3, 2]), scale: torch.Size([3, 2])), 1)\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# getting the tuple (s, a, r, s', done)\n",
    "# ==================================================\n",
    "\n",
    "# action = param.act(states)\n",
    "# param.act calls sample_action_and_compute_log_pi(...) to calculate action\n",
    "\n",
    "# step-by-step run sample_action_and_compute_log_pi\n",
    "mu_given_s = agent.Normal(torch.FloatTensor(s).to(device)) \n",
    "print(mu_given_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4363,  0.9474],\n",
      "        [-1.4536, -1.0624],\n",
      "        [ 0.5099,  1.1436]], device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "u = mu_given_s.rsample() \n",
    "print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4106,  0.7386],\n",
      "        [-0.8964, -0.7866],\n",
      "        [ 0.4699,  0.8156]], device='cuda:0', grad_fn=<TanhBackward>)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tanh(u)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.2542, -1.1955, -1.0898], device='cuda:0', grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "log_pi_a_given_s = mu_given_s.log_prob(u) - (2 * (np.log(2) - u - F.softplus(-2 * u))).sum(dim=1)\n",
    "print(log_pi_a_given_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "print(a.detach().cpu().numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.3415667   0.52769583]\n",
      " [-0.83951175 -0.6423277 ]\n",
      " [ 0.24042882 -0.6538752 ]]\n"
     ]
    }
   ],
   "source": [
    "# finish step-by-step run sample_action_and_compute_log_pi, try call the function directly\n",
    "a, _ = agent.sample_action_and_compute_log_pi(torch.FloatTensor(s).to(device), use_reparametrization_trick=False)\n",
    "a = a.detach().cpu().numpy()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.92267895  0.8063887 ]\n",
      " [-0.95910263  0.051797  ]\n",
      " [-0.98335695 -0.08899456]]\n"
     ]
    }
   ],
   "source": [
    "a = agent.act(torch.FloatTensor(s).to(device))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 8) ,  (3, 1) ,  (3, 1)\n"
     ]
    }
   ],
   "source": [
    "# next_obs, reward, done, _ = env.step(action)\n",
    "NextDecisionSteps, NextTerminalSteps = env.get_steps(behaviorName)\n",
    "ns = NextDecisionSteps.obs[0]\n",
    "reward = NextDecisionSteps.reward\n",
    "reward = np.expand_dims(reward, axis=1)\n",
    "done = np.array([[0]]*N_AGENTS ) \n",
    "print(ns.shape, ', ', reward.shape, ', ', done.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]] \n",
      " 0.0\n"
     ]
    }
   ],
   "source": [
    "print(reward, \"\\n\", np.mean(reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# storing it to the buffer\n",
    "# ==================================================\n",
    "\n",
    "#buf.push(Transition(obs, action, reward, next_obs, done))\n",
    "buf.push(Transition(s, a, reward, ns, done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.92267895  0.8063887 ]\n",
      " [-0.95910263  0.051797  ]\n",
      " [-0.98335695 -0.08899456]] [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "print(buf.memory[0].a, buf.memory[0].r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a loop to fill the buffer with batch_size so we can update NN\n",
    "DecisionSteps, TerminalSteps = env.get_steps(behaviorName)\n",
    "for i in range(2*batch_size):\n",
    "    s = DecisionSteps.obs[0]\n",
    "    a = agent.act(torch.FloatTensor(s).to(device))\n",
    "    NextDecisionSteps, NextTerminalSteps = env.get_steps(behaviorName)\n",
    "    \n",
    "    #if next decision step misses some agents, then reset \n",
    "    if(len(NextDecisionSteps)!= N_AGENTS): \n",
    "        print(i, \" reset training!\")\n",
    "        env.reset()    \n",
    "        DecisionSteps, TerminalSteps = env.get_steps(behaviorName)\n",
    "    else: \n",
    "        ns = NextDecisionSteps.obs[0]\n",
    "        reward = NextDecisionSteps.reward\n",
    "        reward = np.expand_dims(reward, axis=1)\n",
    "        done = np.array([[0]]*N_AGENTS ) \n",
    "        buf.push(Transition(s, a, reward, ns, done))\n",
    "        DecisionSteps, TerminalSteps = NextDecisionSteps, NextTerminalSteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(buf.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 3 4\n",
      "(3, 8) (3, 2) (3, 1) (3, 8) (3, 1)\n"
     ]
    }
   ],
   "source": [
    "# agent.update_networks(buf.sample(batch_size))\n",
    "\n",
    "#step by step to run update_network\n",
    "\n",
    "# buf.sample(batch_size)\n",
    "#Transition = namedtuple('Transition', 's a r ns d')\n",
    "#Batch = namedtuple('Batch', 's a r ns d')\n",
    "experiences = random.sample(buf.memory, batch_size) \n",
    "print(batch_size, N_AGENTS, len(experiences))\n",
    "print(experiences[0].s.shape, experiences[0].a.shape, experiences[0].r.shape, \\\n",
    "      experiences[0].ns.shape, experiences[0].d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 8), (3, 8), (3, 8), (3, 8)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e.s.shape for e in experiences if e is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 8)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack([e.s for e in experiences if e is not None]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 8) (12, 2) (12, 1) (12, 8) (12, 1)\n"
     ]
    }
   ],
   "source": [
    "#Find all states in memory batch, append vertically, convert to torch with float values and send to CPU or GPU\n",
    "bs  = np.vstack([e.s for e in experiences if e is not None])\n",
    "ba  = np.vstack([e.a for e in experiences if e is not None])\n",
    "br  = np.vstack([e.r for e in experiences if e is not None])\n",
    "b_ns = np.vstack([e.ns for e in experiences if e is not None])\n",
    "bd  = np.vstack([e.d for e in experiences if e is not None])\n",
    "print(bs.shape, ba.shape, br.shape, b_ns.shape, bd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 8])\n"
     ]
    }
   ],
   "source": [
    "bs  = torch.from_numpy(np.vstack([e.s for e in experiences if e is not None])).float().to(device)\n",
    "print(bs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 8]) torch.Size([12, 2]) torch.Size([12, 1]) torch.Size([12, 8]) torch.Size([12, 1])\n"
     ]
    }
   ],
   "source": [
    "bs  = torch.from_numpy(np.vstack([e.s for e in experiences if e is not None])).float().to(device)\n",
    "ba  = torch.from_numpy(np.vstack([e.a for e in experiences if e is not None])).float().to(device)\n",
    "br  = torch.from_numpy(np.vstack([e.r for e in experiences if e is not None])).float().to(device)\n",
    "b_ns  = torch.from_numpy(np.vstack([e.ns for e in experiences if e is not None])).float().to(device)\n",
    "bd  = torch.from_numpy(np.vstack([e.d for e in experiences if e is not None])).float().to(device)\n",
    "print(bs.shape, ba.shape, br.shape, b_ns.shape, bd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 8]) torch.Size([12, 2]) torch.Size([12, 1]) torch.Size([12, 8]) torch.Size([12, 1])\n"
     ]
    }
   ],
   "source": [
    "# finish step-by-step test of buf.sample\n",
    "# directly call buf.sample(batch_size)\n",
    "batch = buf.sample(batch_size)\n",
    "print(batch.s.shape, batch.a.shape, batch.r.shape, batch.ns.shape, batch.d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Step 12: calculating targets\n",
    "# ========================================\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    na, log_pi_na_given_ns = agent.sample_action_and_compute_log_pi(b_ns, use_reparametrization_trick=False)\n",
    "    targets = br + agent.gamma * (1 - bd) * \\\n",
    "              (agent.min_i_12(agent.Q1_targ(b_ns, na), agent.Q2_targ(b_ns, na)) - agent.alpha * log_pi_na_given_ns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Step 13: learning the Q functions\n",
    "# ========================================\n",
    "\n",
    "Q1_predictions = agent.Q1(bs, ba)\n",
    "Q1_loss = torch.mean((Q1_predictions - targets) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0176, device='cuda:0', grad_fn=<MeanBackward0>) \n",
      " 0.017580488696694374\n"
     ]
    }
   ],
   "source": [
    "print(Q1_loss, \"\\n\", float(Q1_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.Q1_optimizer.zero_grad()\n",
    "Q1_loss.backward()\n",
    "agent.clip_gradient(net=agent.Q1)\n",
    "agent.Q1_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2_predictions = agent.Q2(bs, ba)\n",
    "Q2_loss = torch.mean((Q2_predictions - targets) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0068, device='cuda:0', grad_fn=<MeanBackward0>) \n",
      " 0.006780985742807388\n"
     ]
    }
   ],
   "source": [
    "print(Q2_loss, \"\\n\", float(Q2_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.Q2_optimizer.zero_grad()\n",
    "Q2_loss.backward()\n",
    "agent.clip_gradient(net=agent.Q2)\n",
    "agent.Q2_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Step 14: learning the policy\n",
    "# ========================================\n",
    "\n",
    "for param in agent.Q1.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in agent.Q2.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, log_pi_a_given_s = agent.sample_action_and_compute_log_pi(bs, use_reparametrization_trick=True)\n",
    "policy_loss = - torch.mean(agent.min_i_12(agent.Q1(bs, a), agent.Q2(bs, a)) - agent.alpha * log_pi_a_given_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.2309, device='cuda:0', grad_fn=<NegBackward>) \n",
      " -0.23088714480400085\n"
     ]
    }
   ],
   "source": [
    "print(policy_loss, \"\\n\", float(policy_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.Normal_optimizer.zero_grad()\n",
    "policy_loss.backward()\n",
    "agent.clip_gradient(net=agent.Normal)\n",
    "agent.Normal_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in agent.Q1.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in agent.Q2.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Step 15: update target networks\n",
    "# ========================================\n",
    "\n",
    "with torch.no_grad():\n",
    "    agent.polyak_update(old_net=agent.Q1_targ, new_net=agent.Q1)\n",
    "    agent.polyak_update(old_net=agent.Q2_targ, new_net=agent.Q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finish step-by-step test of agent.update_network\n",
    "# directly call agent.update_networks(buf.sample(batch_size))\n",
    "agent.update_networks(buf.sample(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "3. NN with policy interacts with 3D Ball to collect training data (MLAgent 10).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
