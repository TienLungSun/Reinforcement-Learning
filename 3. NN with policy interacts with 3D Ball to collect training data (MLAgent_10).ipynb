{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3. NN with policy interacts with 3D Ball to collect training data (MLAgent 10).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TienLungSun/Intelligent-Robot/blob/main/LearnPPO-AC/3.%20NN%20with%20policy%20interacts%20with%203D%20Ball%20to%20collect%20training%20data%20(MLAgent_10).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z29hg89Qmg-S"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal\n",
        "import numpy as np\n",
        "\n",
        "from mlagents_envs.environment import UnityEnvironment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5TAt7Mcmg-W",
        "outputId": "6ef7273c-d02b-45b2-9512-a2e57da25817"
      },
      "source": [
        "if(torch.cuda.is_available()):\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(device, torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device= torch.device(\"cpu\")\n",
        "    print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda GeForce GTX 1660 SUPER\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rHdXcP-mg-X"
      },
      "source": [
        "# state = input of NN, action = output of NN\n",
        "N_STATES  = 8\n",
        "N_ACTIONS =2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEjeAPtXmg-Y"
      },
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
        "        nn.init.constant_(m.bias, 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MJKXhTgmg-Y"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(N_STATES, 128),\n",
        "            nn.LayerNorm(128),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.LayerNorm(128),\n",
        "            nn.Linear(128, N_ACTIONS)\n",
        "        )\n",
        "        self.log_std = nn.Parameter(torch.ones(1, N_ACTIONS) * 0.0)\n",
        "        self.apply(init_weights)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        mu    = self.actor(x)\n",
        "        std   = self.log_std.exp().expand_as(mu)\n",
        "        dist  = Normal(mu, std)\n",
        "        return dist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sj7O_oOUmg-Z"
      },
      "source": [
        "net = Net().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqxXry8NpRFA"
      },
      "source": [
        "### Practice collecting data from agents "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2GNW0jsmg-Z"
      },
      "source": [
        "env = UnityEnvironment(file_name= None, base_port=5004)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SeNL04Fmg-b",
        "outputId": "d0e2735b-933e-438c-b738-fbc9820e53a8"
      },
      "source": [
        "env.reset()\n",
        "behaviorNames = list(env.behavior_specs.keys())\n",
        "behaviorName = behaviorNames[0]\n",
        "print(behaviorName)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3DBall?team=0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aewd7uJZpRFG"
      },
      "source": [
        "NoAgents = 3 #My Unity scene has three training environment\n",
        "NoSteps = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqS1E2DkpRFH",
        "outputId": "0ab33a54-3a8b-42be-8a33-32617a9005d7"
      },
      "source": [
        "#practice to create a [NoAgents, 1] tensor\n",
        "a = torch.FloatTensor([[0]]*NoAgents) \n",
        "print(a, a.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.],\n",
            "        [0.],\n",
            "        [0.]]) torch.Size([3, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVzTmz-EpRFI",
        "outputId": "7f7da0c4-92e4-46e9-e64b-eb4257c982de"
      },
      "source": [
        "lst = [a]*NoSteps\n",
        "print(len(lst), lst[0].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20 torch.Size([3, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Etn8ilfBpRFJ",
        "outputId": "ebcebd74-e0ca-4848-9350-3402f3170377"
      },
      "source": [
        "#practice to create a [NoAgents, NoActions] tensor\n",
        "b = torch.FloatTensor([[0]*N_ACTIONS]*NoAgents) \n",
        "print(b, b.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.]]) torch.Size([3, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JggmBx5WpRFK",
        "outputId": "63bced60-52de-4c3a-c633-ea0871b3b0fa"
      },
      "source": [
        "c = torch.FloatTensor([[0]*N_STATES]*NoAgents) \n",
        "print(c, c.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.]]) torch.Size([3, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjvxS5HOpRFL",
        "outputId": "eda46027-853c-465d-b648-4f205b0055d8"
      },
      "source": [
        "#define list to record interaction information\n",
        "values = rewards = masks = [a]*NoSteps\n",
        "log_probs = actions = [b]*NoSteps\n",
        "states =[c]*NoSteps\n",
        "\n",
        "print(len(log_probs), log_probs[0].shape)\n",
        "print(len(values), values[0].shape)\n",
        "print(len(rewards), rewards[0].shape)\n",
        "print(len(masks), masks[0].shape)\n",
        "print(len(states), states[0].shape)\n",
        "print(len(actions), actions[0].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20 torch.Size([3, 2])\n",
            "20 torch.Size([3, 1])\n",
            "20 torch.Size([3, 1])\n",
            "20 torch.Size([3, 1])\n",
            "20 torch.Size([3, 8])\n",
            "20 torch.Size([3, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNbpHB2bmg-c"
      },
      "source": [
        "#find current agent state in decision and terminal steps\n",
        "DecisionSteps, TerminalSteps = env.get_steps(behaviorName)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moBKAzKwpRFM",
        "outputId": "20686ae3-9e08-4b5e-ed81-d4c753da593b"
      },
      "source": [
        "DecisionStepsAgetnIds = list(DecisionSteps.agent_id)\n",
        "print(DecisionStepsAgetnIds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 1, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rx_QzDhpRFN",
        "outputId": "1c0bc62a-b146-4d64-804e-381eb4939580"
      },
      "source": [
        "#check whether current decision and terminal steps contain all agents\n",
        "DecisionStepsAgetnIds = list(DecisionSteps.agent_id)\n",
        "TerminalStepsAgentIds = list(TerminalSteps.agent_id)\n",
        "lst = DecisionStepsAgetnIds + TerminalStepsAgentIds\n",
        "unique_AgentIDs = list(set(lst)) # set = distinct values in a list\n",
        "print(unique_AgentIDs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 1, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KAh-OqPpRFP",
        "outputId": "deacd958-9160-43a5-8831-e0c7598aca60"
      },
      "source": [
        "if(len(unique_AgentIDs) == NoAgents):\n",
        "    print(\"This step includes all agents\")\n",
        "else:\n",
        "    print(\"This step misses some agents\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This step includes all agents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlRsT5J1pRFQ",
        "outputId": "c8deed25-e36a-4684-c90e-3233212963dd"
      },
      "source": [
        "#reward\n",
        "r = DecisionSteps.reward \n",
        "print(r)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5eISB8MpRFQ",
        "outputId": "65642be3-8cef-4ee2-971f-16e677eee05f"
      },
      "source": [
        "torch.FloatTensor(r).unsqueeze(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [0.],\n",
              "        [0.]])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vd_TgK0RpRFR"
      },
      "source": [
        "#practice data collection\n",
        "s = DecisionSteps.obs[0]  \n",
        "s = torch.FloatTensor(s).to(device)    \n",
        "dist = net(s)\n",
        "a = dist.sample() \n",
        "log_prob = dist.log_prob(a)\n",
        "r = DecisionSteps.reward \n",
        "r = torch.FloatTensor(r).unsqueeze(1)\n",
        "step = 0\n",
        "\n",
        "for idx in range(len(DecisionSteps)):\n",
        "    AgentID = DecisionSteps.agent_id[idx] \n",
        "    states[step][AgentID]=s[idx]\n",
        "    values[step][AgentID]= 0.2 # value will be estimated by critic NN later \n",
        "    actions[step][AgentID]=a[idx]\n",
        "    log_probs[step][AgentID]=log_prob[idx]\n",
        "    rewards[step][AgentID]= r[idx]\n",
        "    masks[step][AgentID]= 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYwgIMnlpRFR"
      },
      "source": [
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJ6yWU6RpRFS"
      },
      "source": [
        "### Now run a loop to collect 20 steps of (s, a, r, s1) data from agents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFTgnhRIpRFS"
      },
      "source": [
        "env = UnityEnvironment(file_name= None, base_port=5004)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmY1LhtIpRFT"
      },
      "source": [
        "env.reset()\n",
        "behaviorNames = list(env.behavior_specs.keys())\n",
        "behaviorName = behaviorNames[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwGOmEj2pRFT",
        "outputId": "fe7fb472-dba0-4086-c452-fb9b99480d1b"
      },
      "source": [
        "# collect (s,a,r,s1) from 20 good interactions with Unity\n",
        "a = torch.FloatTensor([[0]]*NoAgents) #create NoAgents by 1 tensor\n",
        "b = torch.FloatTensor([[0]*N_ACTIONS]*NoAgents) \n",
        "c = torch.FloatTensor([[0]*N_STATES]*NoAgents) \n",
        "values = rewards = masks = [a]*NoSteps\n",
        "log_probs = actions = [b]*NoSteps\n",
        "states =[c]*NoSteps\n",
        "\n",
        "step = 0  #index 0\n",
        "DecisionSteps, TerminalSteps = env.get_steps(behaviorName)\n",
        "while(step < NoSteps): #try to run 20 good steps\n",
        "    if(len(DecisionSteps) == 0): #we have no decision agents\n",
        "        print(\"step \", step, \"has no decision agents !\")\n",
        "        print(\"Decisin agents\", list(DecisionSteps.agent_id))\n",
        "        print(\"Terminal agents\", list(TerminalSteps.agent_id))\n",
        "        print(\"Reset the training environment!\")\n",
        "        env.reset() \n",
        "        DecisionSteps, TerminalSteps = env.get_steps(behaviorName)\n",
        "        #continue next while loop without increase step\n",
        "    else:  # we have decision agents\n",
        "        lst = list(DecisionSteps.agent_id) + list(TerminalSteps.agent_id)\n",
        "        if(len(list(set(lst))) != NoAgents): \n",
        "            #if this step misses some agents, then only interacts with Unity \n",
        "            #but do not collect data\n",
        "            s = DecisionSteps.obs[0]  \n",
        "            s = torch.FloatTensor(s).to(device)    \n",
        "            dist = net(s)\n",
        "            a = dist.sample() \n",
        "            env.set_actions(behaviorName, a.cpu().detach().numpy() )   \n",
        "            env.step()\n",
        "            DecisionSteps, TerminalSteps = env.get_steps(behaviorName)\n",
        "            #continue next while loop without increase step\n",
        "        else: \n",
        "            #this step includes all agents, collect (s, a, r, s1)\n",
        "            s = DecisionSteps.obs[0]  \n",
        "            s = torch.FloatTensor(s).to(device)         \n",
        "            dist = net(s)\n",
        "            a = dist.sample()\n",
        "            log_prob = dist.log_prob(a)\n",
        "            for idx in range(len(DecisionSteps)):\n",
        "                #find decision agents and record their state, value and actions\n",
        "                AgentID = DecisionSteps.agent_id[idx]\n",
        "                states[step][AgentID]=s[idx]\n",
        "                values[step][AgentID]= 0.2 # value will be estimated by critic NN later \n",
        "                actions[step][AgentID]=a[idx]\n",
        "                log_probs[step][AgentID]=log_prob[idx]\n",
        "            env.set_actions(behaviorName, a.cpu().detach().numpy() )   \n",
        "            env.step()\n",
        "            DecisionSteps, TerminalSteps = env.get_steps(behaviorName)\n",
        "            #if next step misses some agents, then do not update step\n",
        "            lst = list(DecisionSteps.agent_id) + list(TerminalSteps.agent_id)\n",
        "            if(len(list(set(lst))) != NoAgents):\n",
        "                continue #skip this step, run next while loop without increase step index\n",
        "            else:\n",
        "                #collect reward of this action from next decision and terminal steps\n",
        "                if(len(TerminalSteps) >0):\n",
        "                    #if next step has terminal agents, then collect terminal agents first\n",
        "                    r = TerminalSteps.reward\n",
        "                    r = torch.FloatTensor(r).unsqueeze(1)\n",
        "                    for idx in range(len(TerminalSteps)):\n",
        "                        AgentID = TerminalSteps.agent_id[idx]\n",
        "                        rewards[step][AgentID]=r[idx]\n",
        "                        masks[step][AgentID]= 0\n",
        "                #collect from decision steps\n",
        "                r = DecisionSteps.reward \n",
        "                r = torch.FloatTensor(r).unsqueeze(1)\n",
        "                for idx in range(len(DecisionSteps)):\n",
        "                    AgentID = DecisionSteps.agent_id[idx] \n",
        "                    if(rewards[step][AgentID] == None): #this agent is not in terminal step\n",
        "                        rewards[step][AgentID]= r[idx]\n",
        "                        masks[step][AgentID]= 1\n",
        "                step = step + 1 #increase step and run next while"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step  17 has no decision agents !\n",
            "Decisin agents []\n",
            "Terminal agents [1]\n",
            "Reset the training environment!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2hMIiMQpRFU"
      },
      "source": [
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mW65NENmg-h",
        "outputId": "4b5c9b0a-dc53-4ae7-fd8d-73f32e0807ad"
      },
      "source": [
        "print(len(log_probs), log_probs[0].shape)\n",
        "print(len(values), values[0].shape)\n",
        "print(len(rewards), rewards[0].shape)\n",
        "print(len(masks), masks[0].shape)\n",
        "print(len(states), states[0].shape)\n",
        "print(len(actions), actions[0].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20 torch.Size([3, 2])\n",
            "20 torch.Size([3, 1])\n",
            "20 torch.Size([3, 1])\n",
            "20 torch.Size([3, 1])\n",
            "20 torch.Size([3, 8])\n",
            "20 torch.Size([3, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-sucNvQpRFV"
      },
      "source": [
        "### Calculate GAE of this training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMXVuhNgpRFV",
        "outputId": "69042381-81ce-4060-85f4-6c5fd4f790c9"
      },
      "source": [
        "for step in reversed(range(NoSteps)):\n",
        "    print(step, end=\", \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0, "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjHPVkHGpRFV"
      },
      "source": [
        "gae = 0\n",
        "gamma=0.99\n",
        "tau=0.95"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktP6mQzrpRFW",
        "outputId": "32fe4c89-261c-4ef1-e906-75e9ab3bc5a1"
      },
      "source": [
        "# we need to know the value of step 21\n",
        "next_v = torch.FloatTensor([[0.2]]*NoAgents) \n",
        "print(next_v)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.2000],\n",
            "        [0.2000],\n",
            "        [0.2000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1RwfqqVpRFW",
        "outputId": "9548c574-464c-4a09-d787-a02e726e11c4"
      },
      "source": [
        "values1 = values + [next_v]\n",
        "print(len(values1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjsqgvB9pRFW",
        "outputId": "8f84668e-82ea-44cb-edf7-feb8a1494498"
      },
      "source": [
        "# try step = 19\n",
        "returns = []\n",
        "step = 19\n",
        "print(rewards[step], values1[step + 1] , masks[step] ,values[step])\n",
        "delta = rewards[step] + gamma * values1[step + 1] * masks[step] - values1[step]\n",
        "print(\"delta =\", delta)\n",
        "gae = delta + gamma * tau * masks[step] * gae\n",
        "print(\"gae = \", gae)\n",
        "returns.insert(0, gae + values1[step])\n",
        "print(\"returns =\", returns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.2000],\n",
            "        [0.2000],\n",
            "        [0.2000]]) tensor([[0.2000],\n",
            "        [0.2000],\n",
            "        [0.2000]]) tensor([[0.2000],\n",
            "        [0.2000],\n",
            "        [0.2000]]) tensor([[0.2000],\n",
            "        [0.2000],\n",
            "        [0.2000]])\n",
            "delta = tensor([[0.0396],\n",
            "        [0.0396],\n",
            "        [0.0396]])\n",
            "gae =  tensor([[0.0470],\n",
            "        [0.0470],\n",
            "        [0.0470]])\n",
            "returns = [tensor([[0.2470],\n",
            "        [0.2470],\n",
            "        [0.2470]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tNwNk1UpRFX"
      },
      "source": [
        "returns = []\n",
        "for step in reversed(range(NoSteps)):\n",
        "    delta = rewards[step] + gamma * values1[step + 1] * masks[step] - values1[step]\n",
        "    gae = delta + gamma * tau * masks[step] * gae\n",
        "    returns.insert(0, gae + values1[step])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xon0PbSPpRFX",
        "outputId": "764f4ec5-dbba-4f0b-c732-95ec151a0364"
      },
      "source": [
        "print(len(returns), returns[0].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20 torch.Size([3, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9Nqwh7ppRFY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}