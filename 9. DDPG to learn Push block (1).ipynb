{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque  \n",
    "from mlagents_envs.environment import UnityEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DDPG agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STATES  = 210  # 105+105\n",
    "N_ACTIONS = 1  # 1 branch with 7 values, move forward/backward, rotate R/L, move R/L \n",
    "N_AGENTS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e3) #int(1e5)        # replay buffer size\n",
    "BATCH_SIZE =  1   #128              # minibatch size\n",
    "GAMMA = 0.99                  # discount factor\n",
    "TAU = 1e-3                    # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4               # learning rate of the actor \n",
    "LR_CRITIC = 1e-4              # learning rate of the critic\n",
    "WEIGHT_DECAY = 0.0            # L2 weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda GeForce GTX 1660 SUPER\n"
     ]
    }
   ],
   "source": [
    "if(torch.cuda.is_available()):\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(device, torch.cuda.get_device_name(0))\n",
    "else: \n",
    "    device= torch.device(\"cpu\")\n",
    "    print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load NN, reply buffer, and Agent class\n",
    "%run \"9. DDPG_NN_and_MemoryBuffer.ipynb\"   \n",
    "%run \"9. DDPG_Agent.ipynb\"       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=N_STATES, action_size=N_ACTIONS, num_agents=N_AGENTS, random_seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step by step to understand the traing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name= None, base_port=5004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PushBlock?team=0\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "behaviorNames = list(env.behavior_specs.keys())\n",
    "behaviorName = behaviorNames[0]\n",
    "print(behaviorName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 210)\n"
     ]
    }
   ],
   "source": [
    "# get initial state  \n",
    "DecisionSteps, TerminalSteps = env.get_steps(behaviorName)\n",
    "s1 = DecisionSteps.obs[0]\n",
    "s2 = DecisionSteps.obs[1]\n",
    "states = np.concatenate((s1, s2), 1)\n",
    "print(states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the training agent for new episode\n",
    "agent.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the initial episode score to zero.\n",
    "agent_scores = np.zeros(N_AGENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================================== <br>\n",
    "actions = agent.act(states) will call Agent class's act method <br>\n",
    "Step by step to run agent.act(states) in Agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n"
     ]
    }
   ],
   "source": [
    "#we create an empty action matrix\n",
    "acts = np.zeros((N_AGENTS, N_ACTIONS))\n",
    "print(acts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actor(\n",
       "  (fc1): Linear(in_features=210, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To set NN in test mode instead of train mode\n",
    "agent.actor_local.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0215], device='cuda:0', grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test: obtain Q(s,a) for agent 1\n",
    "s = states[0,:]\n",
    "s = torch.from_numpy(s).float().to(device)\n",
    "agent.actor_local(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#again, we don't want to calculate gradients because we are only testing!\n",
    "with torch.no_grad():\n",
    "    for agentIdx in range(N_AGENTS):\n",
    "\n",
    "        #forward propagate states in actor local NN to get actions!\n",
    "        s = states[agentIdx,:]\n",
    "        s = torch.from_numpy(s).float().to(device)\n",
    "        acts[agentIdx,:] = agent.actor_local(s).cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#This is to set the model back to training mode, but no training is done here yet!\n",
    "agent.actor_local.train()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.OUNoise object at 0x000002458C8026A0>\n"
     ]
    }
   ],
   "source": [
    "#adding noise for exploration!\n",
    "noise = OUNoise((N_AGENTS, N_ACTIONS), 0)\n",
    "print(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08913055]\n",
      " [ 0.05316845]\n",
      " [-0.09125599]]\n"
     ]
    }
   ],
   "source": [
    "acts += noise.sample()\n",
    "print(acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08913055]\n",
      " [ 0.05316845]\n",
      " [-0.09125599]]\n"
     ]
    }
   ],
   "source": [
    "acts = np.clip(acts, -1, 1)\n",
    "print(acts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================================== <br>\n",
    "finish agent.act(states) in Agent class, go back to training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to convert action value [-1, 1] to index 0~6\n",
    "def GenerateActionIndex(acts):\n",
    "    result = []\n",
    "    for AgentIdx in range(N_AGENTS):\n",
    "        value = acts[AgentIdx][0]\n",
    "        interval = 2/7\n",
    "        lower = -1\n",
    "        for i in range(7):  #action index 0~6\n",
    "            upper = lower + (i+1)*interval\n",
    "            if(value >= lower and value <= upper):\n",
    "                result.append([i])\n",
    "                break\n",
    "            else:\n",
    "                lower = upper\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2]\n",
      " [2]\n",
      " [2]]\n"
     ]
    }
   ],
   "source": [
    "ActionIdxArray = np.array(GenerateActionIndex(acts))\n",
    "print(ActionIdxArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send the actions to Unity\n",
    "env.set_actions(behaviorName, ActionIdxArray)\n",
    "env.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 210)\n"
     ]
    }
   ],
   "source": [
    "# get next states\n",
    "NextDecisionSteps, NextTerminalSteps = env.get_steps(behaviorName)\n",
    "s1 = NextDecisionSteps.obs[0]\n",
    "s2 = NextDecisionSteps.obs[1]\n",
    "next_states = np.concatenate((s1, s2), 1)\n",
    "print(next_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.001 -0.001 -0.001]\n"
     ]
    }
   ],
   "source": [
    "rewards = NextDecisionSteps.reward\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "dones = np.array([[0]]*N_AGENTS ) \n",
    "print(dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2] []\n"
     ]
    }
   ],
   "source": [
    "print(NextDecisionSteps.agent_id, NextTerminalSteps.agent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "for AgentID in  NextTerminalSteps.agent_id:\n",
    "    dones[AgentID] = 1\n",
    "print(dones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================================== <br>\n",
    "agent.step(states, actions, rewards, next_states, dones) <br>\n",
    "Send (S, A, R, S') info to the training agent for replay buffer (memory) and network updates <br>\n",
    "Step by step to run agent.step(states, actions, rewards, next_states, dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save experience / reward\n",
    "\"\"\"7. Store transitions into replay buffer D\"\"\"\n",
    "for agentIdx in range(N_AGENTS):\n",
    "    agent.memory.add(states[agentIdx,:], ActionIdxArray[agentIdx,:], \\\n",
    "                     rewards[agentIdx], next_states[agentIdx,:],\\\n",
    "                     dones[agentIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(agent.memory)\n",
    "# 3 agents, so one step will collect 3 transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn, if enough samples are available in memory\n",
    "\n",
    "#Grab a batch of transitions from memory \n",
    "# (states, actions, rewards, next_states, dones)\n",
    "experiences = agent.memory.sample() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================================== <br>\n",
    "call agent.learn(experiences) from agent.step(...) <br>\n",
    "Step by step to run agent.learn(experiences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 210]) torch.Size([1, 1]) torch.Size([1, 1]) torch.Size([1, 210]) torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "states, actions, rewards, next_states, dones = experiences\n",
    "print(states.shape, actions.shape, rewards.shape, next_states.shape, dones.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0202]], device='cuda:0', grad_fn=<TanhBackward>)\n"
     ]
    }
   ],
   "source": [
    "actions_next = agent.actor_target(next_states)\n",
    "print(actions_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0106]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "Q_targets_next = agent.critic_target(next_states, actions_next)\n",
    "print(Q_targets_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0095]], device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Compute Q targets for current states (y_i)\n",
    "Q_targets = rewards + (GAMMA * Q_targets_next * (1 - dones))\n",
    "print(Q_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute critic loss\n",
    "Q_expected = agent.critic_local(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.5395e-06, device='cuda:0', grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "print(critic_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.critic_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0209]], device='cuda:0', grad_fn=<TanhBackward>)\n",
      "tensor(-0.0113, device='cuda:0', grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------- update actor ---------------------------- #\n",
    "# Compute actor loss\n",
    "actions_pred = agent.actor_local(states)\n",
    "print(actions_pred)\n",
    "actor_loss = -agent.critic_local(states, actions_pred).mean()\n",
    "print(actor_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimize the loss\n",
    "agent.actor_optimizer.zero_grad()\n",
    "actor_loss.backward()\n",
    "agent.actor_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------- update target networks ----------------------- #\n",
    "agent.soft_update(agent.critic_local, agent.critic_target, TAU)\n",
    "agent.soft_update(agent.actor_local, agent.actor_target, TAU)                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================================== <br>\n",
    "finish agent.step(states, actions, rewards, next_states, dones) <br>\n",
    "go back to training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
