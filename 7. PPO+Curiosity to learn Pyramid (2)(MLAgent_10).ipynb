{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2. NN with policy interacts with 3D Ball (MLAgent 10).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TienLungSun/RL-Unity-ML-Agent/blob/main/7.%20PPO%2BCuriosity%20to%20learn%20Pyramid%20(2)(MLAgent_10).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRObB0mXmZhq"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mlagents_envs.environment import UnityEnvironment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTDJ8jn3mZh5",
        "outputId": "2903d42b-7074-42d1-baaa-9fd1da0b86da"
      },
      "source": [
        "if(torch.cuda.is_available()):\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(device, torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device= torch.device(\"cpu\")\n",
        "    print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda NVIDIA GeForce RTX 3060\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTazqw6NApTS"
      },
      "source": [
        "### Global variables are in captial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEwNFJqKApTT"
      },
      "source": [
        "N_STATES  = 172  # 56+56+56+4\n",
        "N_ACTIONS = 1     # 1 branch with 5 VALUES\n",
        "N_AGENTS = 3\n",
        "\n",
        "HIDDEN_UNITS = 512\n",
        "\n",
        "BATCH_SIZE = 64  \n",
        "BUFFER_SIZE = 12000\n",
        "LEARNING_RATE = 0.0003\n",
        "BETA = 0.001\n",
        "EPSILON = 0.2\n",
        "LAMBD = 0.99\n",
        "N_EPOCH = 3\n",
        "\n",
        "GAMMA = 0.99\n",
        "\n",
        "MAX_STEPS = 5000 #50000\n",
        "TIME_HORIZON = 50 #1000 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWj3nbKZApTU"
      },
      "source": [
        "ENV = BEHAVIOR_NAME = NET = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLDhWd3AApTV"
      },
      "source": [
        "# tensor arrays to store (s,a,r,s1) data collected from N_AGENTS in TIME_HORIZON steps\n",
        "a = torch.FloatTensor([[0]]*N_AGENTS ) #create NoAgents by 1 tensor\n",
        "b = torch.FloatTensor([[0]*N_ACTIONS ]*N_AGENTS ) \n",
        "c = torch.FloatTensor([[0]*N_STATES  ]*N_AGENTS ) \n",
        "\n",
        "VALUES =REWARDS = MASKS = [a]*TIME_HORIZON\n",
        "LOG_PROBS = ACTIONS = [b]*TIME_HORIZON\n",
        "STATES = NEXT_STATES = [c]*TIME_HORIZON"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itA_J8PyApTV"
      },
      "source": [
        "### Actor, critic, forward prediction NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGhCm8MkmZiJ"
      },
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
        "        nn.init.constant_(m.bias, 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8myd-uUmZiK"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(N_STATES, HIDDEN_UNITS),\n",
        "            nn.LayerNorm(HIDDEN_UNITS),\n",
        "            nn.Linear(HIDDEN_UNITS, HIDDEN_UNITS),\n",
        "            nn.LayerNorm(HIDDEN_UNITS),\n",
        "            nn.Linear(HIDDEN_UNITS, 1)\n",
        "        )\n",
        "        \n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(N_STATES, HIDDEN_UNITS),\n",
        "            nn.LayerNorm(HIDDEN_UNITS),\n",
        "            nn.Linear(HIDDEN_UNITS, HIDDEN_UNITS),\n",
        "            nn.LayerNorm(HIDDEN_UNITS),\n",
        "            nn.Linear(HIDDEN_UNITS, N_ACTIONS)\n",
        "        )\n",
        "        \n",
        "        self.fNET = nn.Sequential(\n",
        "            nn.Linear(N_STATES, HIDDEN_UNITS),\n",
        "            nn.Linear(HIDDEN_UNITS, HIDDEN_UNITS),\n",
        "            nn.Linear(HIDDEN_UNITS, N_STATES)\n",
        "        )\n",
        "        \n",
        "        self.log_std = nn.Parameter(torch.ones(1, N_ACTIONS) * 0.0)\n",
        "        self.apply(init_weights)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        pred_next_state = self.fNET(x)\n",
        "        value = self.critic(x)\n",
        "        mu    = self.actor(x)\n",
        "        std   = self.log_std.exp().expand_as(mu)\n",
        "        dist  = Normal(mu, std)\n",
        "        return dist, value, pred_next_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B827GoiQmZiL"
      },
      "source": [
        "NET = Net().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PETyAiyfApTW"
      },
      "source": [
        "optimizer = optim.Adam(NET.parameters(), lr=LEARNING_RATE )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hch4ah9eApTX"
      },
      "source": [
        "### function to interacts with Unity NoSteps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJuHmPRiApTX"
      },
      "source": [
        "def Interact_with_Unity_one_step (DecisionSteps):\n",
        "    # DecisionSteps.obs=[(NoAgents, 56), (NoAgents, 56), (NoAgents, 56), (NoAgents, 4)]\n",
        "    # ENV and NET are global variables \n",
        "    s = torch.FloatTensor(DecisionSteps.obs[0])\n",
        "    for i in range(1, 4):\n",
        "        s = torch.cat((s, torch.FloatTensor(DecisionSteps.obs[i])), 1)\n",
        "    dist, value, _ = NET(s.to(device))\n",
        "    a = dist.sample() \n",
        "    log_prob = dist.log_prob(a)\n",
        "    actionCFD = dist.cdf(a)\n",
        "    actionIdx = GenerateActionIndex(a, actionCFD)\n",
        "    ENV.set_actions(BEHAVIOR_NAME, actionIdx)   \n",
        "    ENV.step()\n",
        "    return s, value, a, log_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-roCi09QApTY"
      },
      "source": [
        "def Collect_REWARDS_and_MASKS (step, AgentSteps, flag): \n",
        "    #flag=1:decision, 0: terminal steps\n",
        "    #REWARDS, MASKS, NEXT_STATES are gloable variables\n",
        "    r = AgentSteps.reward\n",
        "    r = torch.FloatTensor(r).unsqueeze(1)\n",
        "    s = torch.FloatTensor(AgentSteps.obs[0])\n",
        "    for i in range(1, 4):\n",
        "        s = torch.cat((s, torch.FloatTensor(AgentSteps.obs[i])), 1)\n",
        "\n",
        "    s = torch.FloatTensor(s).to(device) \n",
        "    for idx in range(len(AgentSteps)):\n",
        "        AgentID = AgentSteps.agent_id[idx]\n",
        "        REWARDS[step][AgentID]=r[idx]\n",
        "        MASKS[step][AgentID]= flag\n",
        "        NEXT_STATES[step][AgentID]=s[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKMdP2OsApTZ"
      },
      "source": [
        "def GenerateActionIndex(action, actionCFD): \n",
        "    actionIdx = []\n",
        "    for agentID in range(N_AGENTS):\n",
        "        agentActionIdx = []\n",
        "        for idx, value in enumerate(action[agentID]):\n",
        "            cfd = float(actionCFD[agentID][idx])\n",
        "            if(cfd<0.2):\n",
        "                agentActionIdx.append(0)\n",
        "            elif(cfd<0.4):\n",
        "                agentActionIdx.append(1)\n",
        "            elif(cfd<0.6):\n",
        "                agentActionIdx.append(2)\n",
        "            elif(cfd<0.8):\n",
        "                agentActionIdx.append(3)\n",
        "            else:\n",
        "                agentActionIdx.append(4)\n",
        "        actionIdx.append(agentActionIdx)\n",
        "    return np.array(actionIdx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zjny6piApTZ"
      },
      "source": [
        "Interact with Unity to collect data<br /> \n",
        "When the agent's Decision period >1, there will be cases where some agents do not have decision steps. We will collect data only when all agents have decision steps, i.e., len(DecisionSteps)==NoAgents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fRm5bKuApTZ"
      },
      "source": [
        "def collect_training_data ():\n",
        "    #ENV, BEHAVIOR_NAME are gloabl variables\n",
        "    #states, ACTIONS, LOG_PROBS, VALUES, REWARDS, MASKS, NEXT_STATES are global variables (tensor array)\n",
        "    step = 0  #index 0\n",
        "    DecisionSteps, TerminalSteps = ENV.get_steps(BEHAVIOR_NAME)\n",
        "    while(step < TIME_HORIZON): #try to run NoSteps good steps\n",
        "        #if we have no decision agents,then continue next loop without increase step\n",
        "        if(len(DecisionSteps) == 0): \n",
        "            ENV.reset() \n",
        "            DecisionSteps, TerminalSteps = ENV.get_steps(BEHAVIOR_NAME)\n",
        "            continue #continue next while loop without increase step\n",
        "        \n",
        "        # Interacts with Unity one step\n",
        "        s, value, a, log_prob = Interact_with_Unity_one_step (DecisionSteps)\n",
        "        NextDecisionSteps, NextTerminalSteps = ENV.get_steps(BEHAVIOR_NAME)\n",
        "\n",
        "        #if this or next decision step misses some agents, then do not collect data\n",
        "        if(len(DecisionSteps)!= N_AGENTS or len(NextDecisionSteps)!= N_AGENTS): \n",
        "            DecisionSteps, TerminalSteps = NextDecisionSteps, NextTerminalSteps\n",
        "            continue      #continue next while loop without increase step\n",
        "        \n",
        "        #else this and next decision steps includes all agents, collect (s, a, r, s1)\n",
        "        for idx in range(len(DecisionSteps)):\n",
        "            #find decision agents and record their state, value and ACTIONS\n",
        "            AgentID = DecisionSteps.agent_id[idx]\n",
        "            STATES[step][AgentID]=s[idx]\n",
        "            VALUES[step][AgentID]=value[idx]\n",
        "            ACTIONS[step][AgentID]=a[idx]\n",
        "            LOG_PROBS[step][AgentID]=log_prob[idx]\n",
        "\n",
        "        #collect reward of this action from next decision and terminal steps\n",
        "        if(len(NextTerminalSteps) >0):\n",
        "            #if next step has terminal agents, then collect terminal agents first\n",
        "            Collect_REWARDS_and_MASKS(step, NextTerminalSteps, 0)\n",
        "        else:  #else collect r and next state from decision steps\n",
        "            Collect_REWARDS_and_MASKS(step, NextDecisionSteps, 1)\n",
        "        \n",
        "        step = step + 1 #increase step and run next while\n",
        "        DecisionSteps, TerminalSteps = NextDecisionSteps, NextTerminalSteps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9utBxEvmZiM"
      },
      "source": [
        "### GAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irNOWp3ZApTa"
      },
      "source": [
        "def compute_gae(next_value):\n",
        "    value1 = VALUES + [next_value.cpu()]\n",
        "    gae = 0\n",
        "    returns = []\n",
        "    for step in reversed(range(TIME_HORIZON )):\n",
        "        delta = REWARDS[step] + GAMMA*value1[step + 1]*MASKS[step]-value1[step]\n",
        "        gae = delta + GAMMA*LAMBD*MASKS[step]*gae\n",
        "        returns.insert(0, gae + VALUES[step])\n",
        "    return returns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb-QnR9IApTa"
      },
      "source": [
        "### PPO optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09OP8EpxApTb"
      },
      "source": [
        "def ppo_iter():\n",
        "    buffer_size = MERGED_STATES.size(0)\n",
        "    for _ in range(buffer_size// BATCH_SIZE ):\n",
        "        rand_ids = np.random.randint(0, buffer_size, BATCH_SIZE )\n",
        "        yield MERGED_STATES[rand_ids, :], MERGED_ACTIONS[rand_ids, :], MERGED_NEXT_STATES[rand_ids, :],\\\n",
        "              MERGED_LOG_PROBS[rand_ids, :], MERGED_RETURNS[rand_ids, :], MERGED_ADVANTAGES[rand_ids, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbOFHw3aApTb"
      },
      "source": [
        "def ppo_curiosity_update():\n",
        "    print(\"epoch:\")\n",
        "    for epoch in range(N_EPOCH):\n",
        "        print(epoch, end = \", \")\n",
        "        for b_s, b_a, b_s_, b_old_LOG_PROBS, b_return, b_advantage in ppo_iter():\n",
        "            dist, value, pred_b_s_ = NET(b_s.to(device))            \n",
        "            forward_loss = (pred_b_s_ - b_s_.to(device)).pow(2).mean()\n",
        "            \n",
        "            #update Actor-Critic\n",
        "            critic_loss = (b_return.to(device) - value).pow(2).mean()\n",
        "            entropy = dist.entropy().mean()\n",
        "            b_a_new = dist.sample()\n",
        "            b_new_LOG_PROBS = dist.log_prob(b_a_new)\n",
        "            ratio = (b_new_LOG_PROBS - b_old_LOG_PROBS.to(device)).exp()\n",
        "            surr1 = ratio * b_advantage.to(device)\n",
        "            surr2 = torch.clamp(ratio, 1.0-EPSILON, 1.0+EPSILON) * b_advantage.to(device)\n",
        "            actor_loss  = - torch.min(surr1, surr2).mean()\n",
        "            loss = (0.5 * critic_loss + actor_loss - 0.001 * entropy) + 0.02*forward_loss\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    return float(critic_loss), float(actor_loss), float(forward_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM7Imen8ApTb"
      },
      "source": [
        "### Try one training iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDAQoSNrmZiO"
      },
      "source": [
        "ENV = UnityEnvironment(file_name= None, base_port=5004)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY2NB7VmmZiR",
        "outputId": "632c116d-fb16-4296-ec9c-1a23dec62d0e"
      },
      "source": [
        "ENV.reset()\n",
        "behavior_names = list(ENV.behavior_specs.keys())\n",
        "BEHAVIOR_NAME = behavior_names[0]\n",
        "print(BEHAVIOR_NAME)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pyramids?team=0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pd2JRD6KApTc",
        "outputId": "0f9db7a3-6c63-4ad5-a296-23a4d554e29e"
      },
      "source": [
        "collect_training_data()\n",
        "print(len(LOG_PROBS), LOG_PROBS[0].shape)\n",
        "print(len(VALUES), VALUES[0].shape)\n",
        "print(len(REWARDS), REWARDS[0].shape)\n",
        "print(len(MASKS), MASKS[0].shape)\n",
        "print(len(STATES), STATES[0].shape)\n",
        "print(len(ACTIONS), ACTIONS[0].shape)\n",
        "print(len(NEXT_STATES), NEXT_STATES[0].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50 torch.Size([3, 1])\n",
            "50 torch.Size([3, 1])\n",
            "50 torch.Size([3, 1])\n",
            "50 torch.Size([3, 1])\n",
            "50 torch.Size([3, 172])\n",
            "50 torch.Size([3, 1])\n",
            "50 torch.Size([3, 172])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm5vB5AWApTc"
      },
      "source": [
        "# send last next state to calculate value\n",
        "_, next_value, _ = NET(NEXT_STATES[-1].to(device)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7AN3XLSApTc"
      },
      "source": [
        "RETURNS = compute_gae(next_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DgQy4shApTd"
      },
      "source": [
        "MERGED_RETURNS   = torch.cat(RETURNS).detach()\n",
        "MERGED_LOG_PROBS = torch.cat(LOG_PROBS).detach()\n",
        "MERGED_VALUES    = torch.cat(VALUES).detach()\n",
        "MERGED_STATES    = torch.cat(STATES) \n",
        "MERGED_NEXT_STATES   = torch.cat(NEXT_STATES) \n",
        "MERGED_ACTIONS   = torch.cat(ACTIONS)\n",
        "MERGED_ADVANTAGES = MERGED_RETURNS - MERGED_VALUES"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fcfPMtqApTd",
        "outputId": "d5f15050-423b-4f6c-85d5-8c5d6e8a1e3e"
      },
      "source": [
        "print(len(MERGED_RETURNS), MERGED_RETURNS[0].shape)\n",
        "print(len(MERGED_LOG_PROBS), MERGED_LOG_PROBS[0].shape)\n",
        "print(len(MERGED_VALUES), MERGED_VALUES[0].shape)\n",
        "print(len(MERGED_STATES), MERGED_STATES[0].shape)\n",
        "print(len(MERGED_NEXT_STATES), MERGED_NEXT_STATES[0].shape)\n",
        "print(len(MERGED_ACTIONS), MERGED_ACTIONS[0].shape)\n",
        "print(len(MERGED_ADVANTAGES), MERGED_ADVANTAGES[0].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "150 torch.Size([1])\n",
            "150 torch.Size([1])\n",
            "150 torch.Size([1])\n",
            "150 torch.Size([172])\n",
            "150 torch.Size([172])\n",
            "150 torch.Size([1])\n",
            "150 torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4IP2oqlApTd",
        "outputId": "cff388df-f2c7-450d-f820-cfb69b69b91e"
      },
      "source": [
        "critic_loss, actor_loss, forward_loss = ppo_curiosity_update()\n",
        "print(critic_loss, actor_loss, forward_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch:\n",
            "0, 1, 2, 137.13279724121094 -10.685394287109375 3.925424337387085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsPx8EqFmZiZ"
      },
      "source": [
        "ENV.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2z_K1TgmZiZ"
      },
      "source": [
        "# Interact with Unity for N steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enqzAfnCmZia"
      },
      "source": [
        "ENV = UnityEnvironment(file_name= None, base_port=5004)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uTR_9wSApTe"
      },
      "source": [
        "ENV.reset()\n",
        "BEHAVIOR_NAME = list(ENV.behavior_specs.keys())\n",
        "BEHAVIOR_NAME = BEHAVIOR_NAME[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuymApgvmZia",
        "outputId": "b0cce321-2193-49f2-e466-c0d6a905fe83"
      },
      "source": [
        "ActorLossLst = []\n",
        "CriticLossLst = []\n",
        "ForwardLossLst = []\n",
        "frame_idx  = 0 \n",
        "\n",
        "while (frame_idx < MAX_STEPS):\n",
        "    print(\"\\nframe idx = \", frame_idx)\n",
        "    print(\"Interacts with Unity to collect training data\")\n",
        "    collect_training_data()\n",
        "    _, next_value, _ = NET(NEXT_STATES[-1].to(device)) \n",
        "    \n",
        "    print(\"Compute GAE of these training data set\")\n",
        "    RETURNS = compute_gae(next_value)\n",
        "    MERGED_RETURNS   = torch.cat(RETURNS).detach()\n",
        "    MERGED_LOG_PROBS = torch.cat(LOG_PROBS).detach()\n",
        "    MERGED_VALUES    = torch.cat(VALUES).detach()\n",
        "    MERGED_STATES    = torch.cat(STATES) \n",
        "    MERGED_NEXT_STATES    = torch.cat(NEXT_STATES) \n",
        "    MERGED_ACTIONS   = torch.cat(ACTIONS)\n",
        "    MERGED_ADVANTAGES = MERGED_RETURNS - MERGED_VALUES\n",
        "    \n",
        "    print(\"Optimize NN with PPO and curiosity\")\n",
        "    critic_loss, actor_loss, forward_loss = ppo_curiosity_update()\n",
        "    CriticLossLst.append(critic_loss)\n",
        "    ActorLossLst.append(actor_loss)\n",
        "    ForwardLossLst.append(forward_loss)\n",
        "\n",
        "    frame_idx += TIME_HORIZON"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "frame idx =  0\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  50\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  100\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  150\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  200\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  250\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  300\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  350\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  400\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  450\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  500\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  550\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  600\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  650\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  700\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  750\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  800\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  850\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  900\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  950\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  1000\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  1050\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  1100\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  1150\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  1200\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  1250\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  1300\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  1350\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  1400\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  1450\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  1500\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  1550\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  1600\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  1650\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  1700\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  1750\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  1800\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  1850\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  1900\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  1950\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  2000\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  2050\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  2100\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  2150\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  2200\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  2250\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  2300\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  2350\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  2400\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  2450\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  2500\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  2550\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  2600\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  2650\n",
            "Interacts with Unity to collect training data\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  2700\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  2750\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  2800\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  2850\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  2900\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  2950\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  3000\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  3050\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  3100\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  3150\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  3200\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  3250\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  3300\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  3350\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  3400\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  3450\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  3500\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  3550\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  3600\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  3650\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  3700\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  3750\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  3800\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  3850\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  3900\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  3950\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  4000\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  4050\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  4100\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  4150\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  4200\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  4250\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  4300\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  4350\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  4400\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  4450\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  4500\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  4550\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  4600\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  4650\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  4700\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  4750\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  4800\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  4850\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  4900\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  4950\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO and curiosity\n",
            "epoch:\n",
            "0, 1, 2, "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksJ0aS0pmZib"
      },
      "source": [
        "ENV.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHTUiW83ApTf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}