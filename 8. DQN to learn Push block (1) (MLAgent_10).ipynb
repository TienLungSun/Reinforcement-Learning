{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GRObB0mXmZhq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mlagents_envs.environment import UnityEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JTDJ8jn3mZh5",
    "outputId": "2903d42b-7074-42d1-baaa-9fd1da0b86da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "if(torch.cuda.is_available()):\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(device, torch.cuda.get_device_name(0))\n",
    "else: \n",
    "    device= torch.device(\"cpu\")\n",
    "    print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Unity to examine behavior names and the state and action design in this training environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name= None, base_port=5004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PushBlock?team=0']\n",
      "PushBlock?team=0 BehaviorSpec(observation_shapes=[(105,), (105,)], action_spec=ActionSpec(continuous_size=0, discrete_branches=(7,)))\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "behaviorNames = list(env.behavior_specs.keys())\n",
    "print(behaviorNames)\n",
    "for behaviorName in behaviorNames:\n",
    "    behavior_spec = env.behavior_specs[behaviorName]\n",
    "    print(behaviorName, behavior_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STATES  = 210  # 105+105\n",
    "N_ACTIONS = 7  # 1 branch with 7 values, move forward/backward, rotate R/L, move R/L \n",
    "N_AGENTS = 3\n",
    "\n",
    "hidden_units = 256 #from ymal file\n",
    "\n",
    "LEARNING_RATE = 0.0003\n",
    "MEMORY_CAPACITY = 500 #10000\n",
    "BATCH_SIZE = 128 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "FGhCm8MkmZiJ"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1 = nn.Linear(N_STATES, hidden_units)\n",
    "        self.layer2 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.out = nn.Linear(hidden_units, N_ACTIONS)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "g8myd-uUmZiK"
   },
   "outputs": [],
   "source": [
    "eval_net = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "B827GoiQmZiL"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(eval_net.parameters(), lr=LEARNING_RATE)\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_net = Net().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 422)\n"
     ]
    }
   ],
   "source": [
    "MEMORY = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))  # (s, a, r, s_) \n",
    "print(MEMORY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9utBxEvmZiM"
   },
   "source": [
    "### Test interaction with Unity to collect transactions to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name= None, base_port=5004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "rY2NB7VmmZiR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PushBlock?team=0\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "behaviorNames = list(env.behavior_specs.keys())\n",
    "behaviorName = behaviorNames[0]\n",
    "print(behaviorName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 210])\n"
     ]
    }
   ],
   "source": [
    "MemoryIdx = 0\n",
    "DecisionSteps, TerminalSteps = env.get_steps(behaviorName)\n",
    "# merge vector observatin, perception \n",
    "s1 = torch.FloatTensor(DecisionSteps.obs[0])\n",
    "s2 = torch.FloatTensor(DecisionSteps.obs[1])\n",
    "s = torch.cat((s1, s2), 1).to(device)\n",
    "print(s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0460, -0.0088,  0.0599,  0.0435, -0.0347,  0.0683, -0.0125],\n",
      "        [ 0.0748, -0.0147,  0.0506,  0.0532, -0.0379,  0.0772, -0.0175],\n",
      "        [ 0.0623, -0.0146,  0.0442,  0.0519, -0.0418,  0.0971, -0.0041]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>) torch.Size([3, 7])\n"
     ]
    }
   ],
   "source": [
    "action = eval_net(s)\n",
    "print(action, action.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5]\n",
      " [5]\n",
      " [5]] (3, 1)\n"
     ]
    }
   ],
   "source": [
    "MaxIdxOfEachAgent = torch.unsqueeze(torch.max(action, 1)[1], 1)\n",
    "ActionIdxArray = MaxIdxOfEachAgent.cpu().data.numpy()\n",
    "print(ActionIdxArray, ActionIdxArray.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send action indices to Unity\n",
    "env.set_actions(behaviorName, ActionIdxArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get next state and reward\n",
    "DecisionSteps, TerminalSteps = env.get_steps(behaviorName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2] []\n"
     ]
    }
   ],
   "source": [
    "print(DecisionSteps.agent_id, TerminalSteps.agent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 210]) (3,)\n"
     ]
    }
   ],
   "source": [
    "# merge vector observatin, perception \n",
    "s1 = torch.FloatTensor(DecisionSteps.obs[0])\n",
    "s2 = torch.FloatTensor(DecisionSteps.obs[1])\n",
    "s_ = torch.cat((s1, s2), 1).to(device)\n",
    "r = DecisionSteps.reward\n",
    "print(s_.shape, r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(422,)\n"
     ]
    }
   ],
   "source": [
    "#get a transition from agent 1\n",
    "transition = np.hstack((s[0].cpu().numpy(), ActionIdxArray[0], r[0], s_[0].cpu().numpy()))\n",
    "print(transition.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all agents transactions to Memory\n",
    "MemoryIdx = 100\n",
    "for agentIdx in range(N_AGENTS):\n",
    "    transition = np.hstack((s[agentIdx].cpu().numpy(), ActionIdxArray[agentIdx], r[agentIdx], s_[agentIdx].cpu().numpy()))\n",
    "    MEMORY[MemoryIdx, :] = transition\n",
    "    MemoryIdx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interact with Unity to fill the memory<br /> \n",
    "When the agent's Decision period >1, there will be cases where some agents do not have decision steps. We will collect data only when all agents have decision steps, i.e., len(DecisionSteps)==NoAgents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_CAPACITY = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name= None, base_port=5004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PushBlock?team=0\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "behaviorNames = list(env.behavior_specs.keys())\n",
    "behaviorName = behaviorNames[0]\n",
    "print(behaviorName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "MemoryIdx = 0\n",
    "env.reset()\n",
    "DecisionSteps, TerminalSteps = env.get_steps(behaviorName)\n",
    "while (MemoryIdx < MEMORY_CAPACITY):\n",
    "    if(len(DecisionSteps)==0):\n",
    "        print(\"Step\", MemoryIdx, \": no decision steps, reset!\")\n",
    "        env.reset()\n",
    "        DecisionSteps, TerminalSteps = env.get_steps(behaviorName)\n",
    "        continue\n",
    "        \n",
    "    #interacts with Unity one step, but collect data only when all agents\n",
    "    #have decision steps\n",
    "    s1 = torch.FloatTensor(DecisionSteps.obs[0])\n",
    "    s2 = torch.FloatTensor(DecisionSteps.obs[1])\n",
    "    s = torch.cat((s1, s2), 1).to(device)\n",
    "    action = eval_net(s)\n",
    "    MaxIdxOfEachAgent = torch.unsqueeze(torch.max(action, 1)[1], 1)\n",
    "    ActionIdxArray = MaxIdxOfEachAgent.cpu().data.numpy()\n",
    "    env.set_actions(behaviorName, ActionIdxArray) \n",
    "    env.step()\n",
    "    NextDecisionSteps, NextTerminalSteps = env.get_steps(behaviorName)\n",
    "    if(len(DecisionSteps)!= N_AGENTS or len(NextDecisionSteps)!= N_AGENTS): \n",
    "        print(MemoryIdx, \"not all agents having decision steps\", \\\n",
    "              DecisionSteps.agent_id, \"next: \", NextDecisionSteps.agent_id)\n",
    "    else:\n",
    "        #after one step, if all agents have decision steps then collect data\n",
    "        #collect reward of this action from next decision and terminal steps\n",
    "        s1 = torch.FloatTensor(NextDecisionSteps.obs[0])\n",
    "        s2 = torch.FloatTensor(NextDecisionSteps.obs[1])\n",
    "        s_ = torch.cat((s1, s2), 1).to(device)\n",
    "        r = NextDecisionSteps.reward\n",
    "        for i in range(N_AGENTS):\n",
    "            transition = np.hstack((s[i].cpu().numpy(), ActionIdxArray[i], \\\n",
    "                                    r[i], s_[i].cpu().numpy()))\n",
    "            MEMORY[MemoryIdx, :] = transition\n",
    "            MemoryIdx += 1\n",
    "            if(MemoryIdx == MEMORY_CAPACITY):\n",
    "                break\n",
    "    DecisionSteps, TerminalSteps = NextDecisionSteps, NextTerminalSteps  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "2. NN with policy interacts with 3D Ball (MLAgent 10).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
