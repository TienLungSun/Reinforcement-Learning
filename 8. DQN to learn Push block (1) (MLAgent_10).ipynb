{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TienLungSun/RL-Unity-ML-Agent/blob/main/8.%20DQN%20to%20learn%20Push%20block%20(1)%20(MLAgent_10).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRObB0mXmZhq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mlagents_envs.environment import UnityEnvironment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTDJ8jn3mZh5",
        "outputId": "2903d42b-7074-42d1-baaa-9fd1da0b86da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda NVIDIA GeForce RTX 3060\n"
          ]
        }
      ],
      "source": [
        "if(torch.cuda.is_available()):\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(device, torch.cuda.get_device_name(0))\n",
        "else: \n",
        "    device= torch.device(\"cpu\")\n",
        "    print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdGvJR6J5NCc"
      },
      "source": [
        "### Connect to Unity to examine behavior names and the state and action design in this training environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2ZMRKyH5NCd"
      },
      "outputs": [],
      "source": [
        "env = UnityEnvironment(file_name= None, base_port=5004)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLSf_MQL5NCe",
        "outputId": "33eb3a4f-8c0a-4a95-e743-b5fe7a34a764"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['PushBlock?team=0']\n",
            "PushBlock?team=0 BehaviorSpec(observation_shapes=[(105,), (105,)], action_spec=ActionSpec(continuous_size=0, discrete_branches=(7,)))\n"
          ]
        }
      ],
      "source": [
        "env.reset()\n",
        "behaviorNames = list(env.behavior_specs.keys())\n",
        "print(behaviorNames)\n",
        "for behaviorName in behaviorNames:\n",
        "    behavior_spec = env.behavior_specs[behaviorName]\n",
        "    print(behaviorName, behavior_spec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFu1Y2qL5NCe"
      },
      "outputs": [],
      "source": [
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpjJgP4f5NCf"
      },
      "source": [
        "### DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJieqmnN5NCf"
      },
      "outputs": [],
      "source": [
        "N_STATES  = 210  # 105+105\n",
        "N_ACTIONS = 7  # 1 branch with 7 values, move forward/backward, rotate R/L, move R/L \n",
        "N_AGENTS = 3\n",
        "\n",
        "hidden_units = 256 #from ymal file\n",
        "\n",
        "LEARNING_RATE = 0.0003\n",
        "MEMORY_CAPACITY = 500 #10000\n",
        "BATCH_SIZE = 128 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGhCm8MkmZiJ"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super(Net, self).__init__()\n",
        "        self.layer1 = nn.Linear(N_STATES, hidden_units)\n",
        "        self.layer2 = nn.Linear(hidden_units, hidden_units)\n",
        "        self.out = nn.Linear(hidden_units, N_ACTIONS)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        return self.out(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8myd-uUmZiK"
      },
      "outputs": [],
      "source": [
        "eval_net = Net().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B827GoiQmZiL"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(eval_net.parameters(), lr=LEARNING_RATE)\n",
        "loss_func = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYoRJ09q5NCi"
      },
      "outputs": [],
      "source": [
        "target_net = Net().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RON10DyO5NCi"
      },
      "source": [
        "### Replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xz14ts45NCi",
        "outputId": "bb42ce2b-75d0-41ce-a26a-53ff97ccb24d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(500, 422)\n"
          ]
        }
      ],
      "source": [
        "MEMORY = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))  # (s, a, r, s_) \n",
        "print(MEMORY.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9utBxEvmZiM"
      },
      "source": [
        "### Test interaction with Unity to collect transactions to memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cHUGn925NCj"
      },
      "outputs": [],
      "source": [
        "env = UnityEnvironment(file_name= None, base_port=5004)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rY2NB7VmmZiR",
        "outputId": "c6f0c217-0b21-4cc5-da70-077e637269c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PushBlock?team=0\n"
          ]
        }
      ],
      "source": [
        "env.reset()\n",
        "behaviorNames = list(env.behavior_specs.keys())\n",
        "behaviorName = behaviorNames[0]\n",
        "print(behaviorName)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kct0cB8Z5NCj",
        "outputId": "948cdd56-3b80-4b13-aaab-f127ee378ce6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 210])\n"
          ]
        }
      ],
      "source": [
        "MemoryIdx = 0\n",
        "DecisionSteps, TerminalSteps = env.get_steps(behaviorName)\n",
        "# merge vector observatin, perception \n",
        "s1 = torch.FloatTensor(DecisionSteps.obs[0])\n",
        "s2 = torch.FloatTensor(DecisionSteps.obs[1])\n",
        "s = torch.cat((s1, s2), 1).to(device)\n",
        "print(s.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRUv2E2F5NCk",
        "outputId": "cdda9bda-7455-4d5f-cf06-88e390684904"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0460, -0.0088,  0.0599,  0.0435, -0.0347,  0.0683, -0.0125],\n",
            "        [ 0.0748, -0.0147,  0.0506,  0.0532, -0.0379,  0.0772, -0.0175],\n",
            "        [ 0.0623, -0.0146,  0.0442,  0.0519, -0.0418,  0.0971, -0.0041]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>) torch.Size([3, 7])\n"
          ]
        }
      ],
      "source": [
        "action = eval_net(s)\n",
        "print(action, action.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pATw3yrU5NCk",
        "outputId": "833205e8-8cd5-48b2-ff80-6295d3df2245"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[5]\n",
            " [5]\n",
            " [5]] (3, 1)\n"
          ]
        }
      ],
      "source": [
        "MaxIdxOfEachAgent = torch.unsqueeze(torch.max(action, 1)[1], 1)\n",
        "ActionIdxArray = MaxIdxOfEachAgent.cpu().data.numpy()\n",
        "print(ActionIdxArray, ActionIdxArray.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYq5yjp85NCk"
      },
      "outputs": [],
      "source": [
        "# send action indices to Unity\n",
        "env.set_actions(behaviorName, ActionIdxArray)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBPLqDRx5NCl"
      },
      "outputs": [],
      "source": [
        "env.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Py39WNsB5NCl"
      },
      "outputs": [],
      "source": [
        "# get next state and reward\n",
        "DecisionSteps, TerminalSteps = env.get_steps(behaviorName)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvkV3goW5NCl",
        "outputId": "6bf734e2-9c9c-41f7-92f9-38317f4abf2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 1 2] []\n"
          ]
        }
      ],
      "source": [
        "print(DecisionSteps.agent_id, TerminalSteps.agent_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x--S40nF5NCl",
        "outputId": "3d5ca0ea-3e90-4160-f049-4e130fac6929"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 210]) (3,)\n"
          ]
        }
      ],
      "source": [
        "# merge vector observatin, perception \n",
        "s1 = torch.FloatTensor(DecisionSteps.obs[0])\n",
        "s2 = torch.FloatTensor(DecisionSteps.obs[1])\n",
        "s_ = torch.cat((s1, s2), 1).to(device)\n",
        "r = DecisionSteps.reward\n",
        "print(s_.shape, r.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uspauIiA5NCm",
        "outputId": "7fa2871e-c8a7-4ac5-8a24-94972d8ad05e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(422,)\n"
          ]
        }
      ],
      "source": [
        "#get a transition from agent 1\n",
        "transition = np.hstack((s[0].cpu().numpy(), ActionIdxArray[0], r[0], s_[0].cpu().numpy()))\n",
        "print(transition.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tni8JEmO5NCm"
      },
      "outputs": [],
      "source": [
        "# save all agents transactions to Memory\n",
        "MemoryIdx = 100\n",
        "for agentIdx in range(N_AGENTS):\n",
        "    transition = np.hstack((s[agentIdx].cpu().numpy(), ActionIdxArray[agentIdx], r[agentIdx], s_[agentIdx].cpu().numpy()))\n",
        "    MEMORY[MemoryIdx, :] = transition\n",
        "    MemoryIdx += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLXg7JJ85NCm"
      },
      "outputs": [],
      "source": [
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pt2wx7Jo5NCm"
      },
      "source": [
        "Interact with Unity to fill the memory<br /> \n",
        "When the agent's Decision period >1, there will be cases where some agents do not have decision steps. We will collect data only when all agents have decision steps, i.e., len(DecisionSteps)==NoAgents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zc2wRdRJ5NCm"
      },
      "outputs": [],
      "source": [
        "MEMORY_CAPACITY = 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TW2ua76r5NCn"
      },
      "outputs": [],
      "source": [
        "env = UnityEnvironment(file_name= None, base_port=5004)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsElfrMh5NCn",
        "outputId": "70f448fe-b046-4d89-e66c-373d5b9b75ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PushBlock?team=0\n"
          ]
        }
      ],
      "source": [
        "env.reset()\n",
        "behaviorNames = list(env.behavior_specs.keys())\n",
        "behaviorName = behaviorNames[0]\n",
        "print(behaviorName)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7y07Viy5NCn"
      },
      "outputs": [],
      "source": [
        "MemoryIdx = 0\n",
        "env.reset()\n",
        "DecisionSteps, TerminalSteps = env.get_steps(behaviorName)\n",
        "while (MemoryIdx < MEMORY_CAPACITY):\n",
        "    if(len(DecisionSteps)==0):\n",
        "        print(\"Step\", MemoryIdx, \": no decision steps, reset!\")\n",
        "        env.reset()\n",
        "        DecisionSteps, TerminalSteps = env.get_steps(behaviorName)\n",
        "        continue\n",
        "        \n",
        "    #interacts with Unity one step, but collect data only when all agents\n",
        "    #have decision steps\n",
        "    s1 = torch.FloatTensor(DecisionSteps.obs[0])\n",
        "    s2 = torch.FloatTensor(DecisionSteps.obs[1])\n",
        "    s = torch.cat((s1, s2), 1).to(device)\n",
        "    action = eval_net(s)\n",
        "    MaxIdxOfEachAgent = torch.unsqueeze(torch.max(action, 1)[1], 1)\n",
        "    ActionIdxArray = MaxIdxOfEachAgent.cpu().data.numpy()\n",
        "    env.set_actions(behaviorName, ActionIdxArray) \n",
        "    env.step()\n",
        "    NextDecisionSteps, NextTerminalSteps = env.get_steps(behaviorName)\n",
        "    if(len(DecisionSteps)!= N_AGENTS or len(NextDecisionSteps)!= N_AGENTS): \n",
        "        print(MemoryIdx, \"not all agents having decision steps\", \\\n",
        "              DecisionSteps.agent_id, \"next: \", NextDecisionSteps.agent_id)\n",
        "    else:\n",
        "        #after one step, if all agents have decision steps then collect data\n",
        "        #collect reward of this action from next decision and terminal steps\n",
        "        s1 = torch.FloatTensor(NextDecisionSteps.obs[0])\n",
        "        s2 = torch.FloatTensor(NextDecisionSteps.obs[1])\n",
        "        s_ = torch.cat((s1, s2), 1).to(device)\n",
        "        r = NextDecisionSteps.reward\n",
        "        for i in range(N_AGENTS):\n",
        "            transition = np.hstack((s[i].cpu().numpy(), ActionIdxArray[i], \\\n",
        "                                    r[i], s_[i].cpu().numpy()))\n",
        "            MEMORY[MemoryIdx, :] = transition\n",
        "            MemoryIdx += 1\n",
        "            if(MemoryIdx == MEMORY_CAPACITY):\n",
        "                break\n",
        "    DecisionSteps, TerminalSteps = NextDecisionSteps, NextTerminalSteps  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGdRqv615NCn"
      },
      "outputs": [],
      "source": [
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzRTSYx25NCn"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "2. NN with policy interacts with 3D Ball (MLAgent 10).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}