{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3. NN with policy interacts with 3D Ball to collect training data (MLAgent 10).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TienLungSun/Intelligent-Robot/blob/main/LearnPPO-AC/6.1.%20A2C%20use%20Tensorboard%20(MLAgent_10).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z29hg89Qmg-S"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mlagents_envs.environment import UnityEnvironment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5TAt7Mcmg-W",
        "outputId": "6ef7273c-d02b-45b2-9512-a2e57da25817"
      },
      "source": [
        "if(torch.cuda.is_available()):\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(device, torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device= torch.device(\"cpu\")\n",
        "    print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda GeForce GTX 1660 SUPER\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpF8Y91LkRsE"
      },
      "source": [
        "### Training parameters (in Captical letters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rHdXcP-mg-X"
      },
      "source": [
        "N_STATES  = 8\n",
        "N_ACTIONS =2\n",
        "\n",
        "N_AGENTS = 3   #My Unity scene has three training environment\n",
        "\n",
        "BATCH_SIZE = 64  \n",
        "BUFFER_SIZE = 12000\n",
        "LEARNING_RATE = 0.0003\n",
        "BETA = 0.001\n",
        "EPSILON = 0.2\n",
        "LAMBD = 0.99\n",
        "N_EPOCH = 3\n",
        "\n",
        "GAMMA = 0.99\n",
        "\n",
        "MAX_STEPS = 5000 #50000\n",
        "TIME_HORIZON = 50 #1000 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEjeAPtXmg-Y"
      },
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
        "        nn.init.constant_(m.bias, 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MJKXhTgmg-Y"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(N_STATES, 128),\n",
        "            nn.LayerNorm(128),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.LayerNorm(128),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "        \n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(N_STATES, 128),\n",
        "            nn.LayerNorm(128),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.LayerNorm(128),\n",
        "            nn.Linear(128, N_ACTIONS)\n",
        "        )\n",
        "        self.log_std = nn.Parameter(torch.ones(1, N_ACTIONS) * 0.0)\n",
        "        self.apply(init_weights)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        value = self.critic(x)\n",
        "        mu    = self.actor(x)\n",
        "        std   = self.log_std.exp().expand_as(mu)\n",
        "        dist  = Normal(mu, std)\n",
        "        return dist, value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sj7O_oOUmg-Z"
      },
      "source": [
        "net = Net().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmRPSJY2kRsI"
      },
      "source": [
        "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUkwuBJjkRsJ"
      },
      "source": [
        "### function to interacts with Unity NoSteps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJJsgSunkRsK"
      },
      "source": [
        "def collect_training_data (NoAgents, NoSteps, NoStates, NoActions, env, net):\n",
        "    # collect (s,a,r,s1) from 20 good interactions with Unity\n",
        "    a = torch.FloatTensor([[0]]*NoAgents) #create NoAgents by 1 tensor\n",
        "    b = torch.FloatTensor([[0]*NoActions]*NoAgents) \n",
        "    c = torch.FloatTensor([[0]*NoStates]*NoAgents) \n",
        "    values =rewards = masks = [a]*NoSteps\n",
        "    log_probs = actions = [b]*NoSteps\n",
        "    states =[c]*NoSteps\n",
        "    next_state = c\n",
        "\n",
        "    step = 0  #index 0\n",
        "    DecisionSteps, TerminalSteps = env.get_steps(behaviorName)\n",
        "    while(step < NoSteps): #try to run 20 good steps\n",
        "        if(len(DecisionSteps) == 0): #we have no decision agents\n",
        "            env.reset() \n",
        "            DecisionSteps, TerminalSteps = env.get_steps(behaviorName)\n",
        "            #continue next while loop without increase step\n",
        "        else:  # we have decision agents\n",
        "            lst = list(DecisionSteps.agent_id) + list(TerminalSteps.agent_id)\n",
        "            if(len(list(set(lst))) != NoAgents): \n",
        "                #if this step misses some agents, then only interacts with Unity \n",
        "                #but do not collect data\n",
        "                s = DecisionSteps.obs[0]  \n",
        "                s = torch.FloatTensor(s).to(device)    \n",
        "                dist, value = net(s)\n",
        "                a = dist.sample() \n",
        "                env.set_actions(behaviorName, a.cpu().detach().numpy() )   \n",
        "                env.step()\n",
        "                DecisionSteps, TerminalSteps = env.get_steps(behaviorName)\n",
        "                #continue next while loop without increase step\n",
        "            else: \n",
        "                #this step includes all agents, collect (s, a, r, s1)\n",
        "                s = DecisionSteps.obs[0]  \n",
        "                s = torch.FloatTensor(s).to(device)         \n",
        "                dist, value = net(s)\n",
        "                a = dist.sample()\n",
        "                log_prob = dist.log_prob(a)\n",
        "                for idx in range(len(DecisionSteps)):\n",
        "                    #find decision agents and record their state, value and actions\n",
        "                    AgentID = DecisionSteps.agent_id[idx]\n",
        "                    states[step][AgentID]=s[idx]\n",
        "                    values[step][AgentID]=value[idx]\n",
        "                    actions[step][AgentID]=a[idx]\n",
        "                    log_probs[step][AgentID]=log_prob[idx]\n",
        "                env.set_actions(behaviorName, a.cpu().detach().numpy() )   \n",
        "                env.step()\n",
        "                DecisionSteps, TerminalSteps = env.get_steps(behaviorName)\n",
        "                #if next step misses some agents, then do not update step\n",
        "                lst = list(DecisionSteps.agent_id) + list(TerminalSteps.agent_id)\n",
        "                if(len(list(set(lst))) != NoAgents):\n",
        "                    continue #skip this step, run next while loop without increase step index\n",
        "                else:\n",
        "                    #collect reward of this action from next decision and terminal steps\n",
        "                    if(len(TerminalSteps) >0):\n",
        "                        #if next step has terminal agents, then collect terminal agents first\n",
        "                        r = TerminalSteps.reward\n",
        "                        r = torch.FloatTensor(r).unsqueeze(1)\n",
        "                        s = TerminalSteps.obs[0]  \n",
        "                        s = torch.FloatTensor(s).to(device) \n",
        "                        for idx in range(len(TerminalSteps)):\n",
        "                            AgentID = TerminalSteps.agent_id[idx]\n",
        "                            rewards[step][AgentID]=r[idx]\n",
        "                            masks[step][AgentID]= 0\n",
        "                            next_state[AgentID]=s[idx]\n",
        "                    #collect from decision steps\n",
        "                    r = DecisionSteps.reward \n",
        "                    r = torch.FloatTensor(r).unsqueeze(1)\n",
        "                    s = DecisionSteps.obs[0]  \n",
        "                    s = torch.FloatTensor(s).to(device) \n",
        "                    for idx in range(len(DecisionSteps)):\n",
        "                        AgentID = DecisionSteps.agent_id[idx] \n",
        "                        if(rewards[step][AgentID] == None): #this agent is not in terminal step\n",
        "                            rewards[step][AgentID]= r[idx]\n",
        "                            masks[step][AgentID]= 1\n",
        "                            next_state[AgentID]=s[idx]\n",
        "                    step = step + 1 #increase step and run next while\n",
        "    return  states, actions, log_probs, values, rewards, masks, next_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmKw3NaakRsL"
      },
      "source": [
        "### GAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxfSVYF3kRsM"
      },
      "source": [
        "def compute_gae(NoSteps, next_value, rewards, masks, values, gamma, tau):\n",
        "    values = values + [next_value.cpu()]\n",
        "    gae = 0\n",
        "    returns = []\n",
        "    for step in reversed(range(NoSteps)):\n",
        "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
        "        gae = delta + gamma * tau * masks[step] * gae\n",
        "        returns.insert(0, gae + values[step])\n",
        "    return returns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSwkp0l3kRsM"
      },
      "source": [
        "### PPO optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pm7_twkUkRsM"
      },
      "source": [
        "def ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantages):\n",
        "    batch_size = states.size(0)\n",
        "    for _ in range(batch_size // mini_batch_size):\n",
        "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
        "        yield states[rand_ids, :], actions[rand_ids, :], log_probs[rand_ids, :], returns[rand_ids, :], advantages[rand_ids, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EncYvDOhkRsN"
      },
      "source": [
        "def ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, clip_param=0.2):\n",
        "    print(\"epoch:\")\n",
        "    for epoch in range(ppo_epochs):\n",
        "        print(epoch, end = \", \")\n",
        "        for batch_state, batch_action, batch_old_log_probs, batch_return, batch_advantage in ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantages):\n",
        "            dist, value = net(batch_state.to(device))\n",
        "            critic_loss = (batch_return.to(device) - value).pow(2).mean()\n",
        "            entropy = dist.entropy().mean()\n",
        "            batch_action = dist.sample()\n",
        "            batch_new_log_probs = dist.log_prob(batch_action)\n",
        "            ratio = (batch_new_log_probs - batch_old_log_probs.to(device)).exp()\n",
        "            surr1 = ratio * batch_advantage.to(device)\n",
        "            surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * batch_advantage.to(device)\n",
        "            actor_loss  = - torch.min(surr1, surr2).mean()\n",
        "            loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    return float(critic_loss), float(actor_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Q296GtYkRsN"
      },
      "source": [
        "### Start Tensorboard server"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Q8iFl3QykRsN",
        "outputId": "85df6954-ff5b-4ebe-eff1-318103f4a2b9"
      },
      "source": [
        "# current directory of this ipython notebook \n",
        "!echo The current directory is %CD%"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The current directory is C:\\Users\\ADMIN\\Google 雲端硬碟\\0. 教學 IE 351, 562 VR1 Unity 3D\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jp1vp1AkRsO"
      },
      "source": [
        "# Run python terminal window\n",
        "# Change directory to this ipython notebook directory\n",
        "# tensorboard --logdir=runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5M4thxrckRsO"
      },
      "source": [
        "# Web browser:  localhost:6006 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A1AvAW6kRsO"
      },
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yffTUfNLkRsP"
      },
      "source": [
        "# run PPO optimization multiple iterations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWeR2eorkRsP"
      },
      "source": [
        "BATCH_SIZE = 64  \n",
        "BUFFER_SIZE = 12000\n",
        "N_EPOCH = 3\n",
        "\n",
        "BETA = 0.001\n",
        "EPSILON = 0.2\n",
        "LAMBD = 0.99\n",
        "GAMMA = 0.99\n",
        "\n",
        "MAX_STEPS = 500 #50000\n",
        "TIME_HORIZON = 50 #1000 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrHKSwgekRsP"
      },
      "source": [
        "env = UnityEnvironment(file_name= None, base_port=5004)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yctPzRKukRsQ",
        "outputId": "4ec427a9-eab6-4b85-adae-eb19b1a3eb42"
      },
      "source": [
        "env.reset()\n",
        "behaviorNames = list(env.behavior_specs.keys())\n",
        "behaviorName = behaviorNames[0]\n",
        "print(behaviorName)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3DBall?team=0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1i7uU0EkRsQ",
        "outputId": "dd2ad62e-5694-430c-c8c1-832a73eefab7"
      },
      "source": [
        "frame_idx  = 0 \n",
        "\n",
        "while (frame_idx < MAX_STEPS):\n",
        "    print(\"\\nframe idx = \", frame_idx)\n",
        "    print(\"Interacts with Unity to collect training data\")\n",
        "    states, actions, log_probs, values, rewards, masks, next_state = collect_training_data (N_AGENTS, TIME_HORIZON, N_STATES, N_ACTIONS, env, net)\n",
        "    _, next_value = net(next_state.to(device)) \n",
        "    \n",
        "    print(\"Compute GAE of these training data set\")\n",
        "    returns = compute_gae(TIME_HORIZON, next_value, rewards, masks, values, GAMMA, LAMBD)\n",
        "\n",
        "    returns   = torch.cat(returns).detach()\n",
        "    log_probs = torch.cat(log_probs).detach()\n",
        "    values    = torch.cat(values).detach()\n",
        "    states    = torch.cat(states) \n",
        "    actions   = torch.cat(actions)\n",
        "    advantages = returns - values\n",
        "    \n",
        "    print(\"Optimize NN with PPO\")\n",
        "    critic_loss, actor_loss = ppo_update(N_EPOCH, BATCH_SIZE, states, actions, log_probs, returns, advantages, clip_param=0.2)    \n",
        "    writer.add_scalar(\"Actor loss\", actor_loss, frame_idx)\n",
        "    writer.add_scalar(\"Critic loss\", critic_loss, frame_idx)\n",
        "\n",
        "    frame_idx += TIME_HORIZON"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "frame idx =  0\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  50\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  100\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  150\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  200\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  250\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  300\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  350\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  400\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO\n",
            "epoch:\n",
            "0, 1, 2, \n",
            "frame idx =  450\n",
            "Interacts with Unity to collect training data\n",
            "Compute GAE of these training data set\n",
            "Optimize NN with PPO\n",
            "epoch:\n",
            "0, 1, 2, "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ATHajTfkRsQ"
      },
      "source": [
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiwkEHRbkRsQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}