{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    #initialize an Agent object\n",
    "    def __init__(self, state_size, action_size, num_agents, random_seed):\n",
    "        \n",
    "        #set dimension of state vector size\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        #set dimension of action vector size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        #set number of agents (cores or workers)\n",
    "        self.num_agents = num_agents\n",
    "        \n",
    "        #set to reproduce results\n",
    "        self.seed = random.seed(random_seed)\n",
    "        \n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "        \n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # Add noise to the actions of the agent to ensure exploration using Ornstein Uhlenbeck noise process\n",
    "        self.noise = OUNoise((num_agents, action_size), random_seed)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def step(self, states, actions, rewards, next_states, dones):\n",
    "        \n",
    "        # Save experience / reward\n",
    "        for agent in range(self.num_agents):\n",
    "            self.memory.add(states[agent,:], actions[agent,:], rewards[agent], next_states[agent,:], dones[agent])\n",
    "        \n",
    "        # Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            \n",
    "            #Grab a batch of transitions from memory\n",
    "            experiences = self.memory.sample()\n",
    "            \n",
    "            #Here BEGINS STEP 3! We will learn using our batch of transitions!\n",
    "            self.learn(experiences)\n",
    "            \n",
    "            \n",
    "    #Returns actions for given state as per current policy.\n",
    "    def act(self, state, add_noise=True):\n",
    "        \n",
    "        #we prepare our state\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        \n",
    "        #we create an empty action matrix\n",
    "        acts = np.zeros((self.num_agents, self.action_size))\n",
    "        \n",
    "        #To set NN in test mode instead of train mode\n",
    "        self.actor_local.eval()\n",
    "        \n",
    "        #again, we don't want to calculate gradients because we are only testing!\n",
    "        with torch.no_grad():\n",
    "            for agent in range(self.num_agents):\n",
    "                \n",
    "                #forward propagate states in actor local NN to get actions!\n",
    "                acts[agent,:] = self.actor_local(state[agent,:]).cpu().data.numpy()\n",
    "        \n",
    "        #This is to set the model back to training mode, but no training is done here yet!\n",
    "        self.actor_local.train()\n",
    "        \n",
    "        #adding noise for exploration!\n",
    "        if add_noise:\n",
    "            acts += self.noise.sample()\n",
    "        return np.clip(acts, -1, 1)\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "        \n",
    "    def learn(self, experiences):\n",
    "        #  Update policy and value parameters using given batch of experience tuples.\n",
    "        #  Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        #  where:\n",
    "        #      actor_target(state) -> action\n",
    "        #      critic_target(state, action) -> Q-value\n",
    "        #  Params\n",
    "        #  ======\n",
    "        #      experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "        #      gamma (float): discount factor\n",
    "        \n",
    "        #  states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        actions_next = self.actor_target(next_states) \n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        \n",
    "        # Compute Q targets for current states (y_i)\n",
    "        \"\"\"12. calculate 'yi' \"\"\"\n",
    "        Q_targets = rewards + (GAMMA * Q_targets_next * (1 - dones))\n",
    "        \n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        \n",
    "        #torch.nn.utils.clip_grad_norm(self.critic_local.parameters(), 1)\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        \n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0.0, theta=0.15, sigma=0.15, sigma_min = 0.05, sigma_decay=.975):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.sigma_min = sigma_min\n",
    "        self.sigma_decay = sigma_decay\n",
    "        self.seed = random.seed(seed)\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "        \"\"\"Resduce  sigma from initial value to min\"\"\"\n",
    "        self.sigma = max(self.sigma_min, self.sigma*self.sigma_decay)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.standard_normal(self.size)\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
